{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51662c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "from collections import Counter   # 用來方便累加每個 chunk 的統計結果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e2aaea",
   "metadata": {},
   "source": [
    "# finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873ae102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00_setup_os處理函數\n",
    "def create_folder(folder_name):\n",
    "    \"\"\"建立資料夾\"\"\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    return os.path.abspath(folder_name)\n",
    "\n",
    "def findfiles(filefolderpath, filetype='.csv', recursive=True):\n",
    "    \"\"\"\n",
    "    尋找指定路徑下指定類型的檔案，並返回檔案路徑列表。\n",
    "\n",
    "    Args:\n",
    "        filefolderpath (str): 指定的檔案路徑。\n",
    "        filetype (str, optional): 要尋找的檔案類型，預設為 '.csv'。\n",
    "        recursive (bool, optional): 是否檢索所有子資料夾，預設為 True；反之為False，僅查找當前資料夾的所有file。\n",
    "\n",
    "    Returns:\n",
    "        list: 包含所有符合條件的檔案路徑的列表。\n",
    "    \"\"\"\n",
    "    filelist = []\n",
    "\n",
    "    if recursive:\n",
    "        # 遍歷資料夾及其子資料夾\n",
    "        for root, _, files in os.walk(filefolderpath):\n",
    "            for file in files:\n",
    "                if file.endswith(filetype):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    filelist.append(file_path)\n",
    "    else:\n",
    "        # 僅檢索當前資料夾\n",
    "        for file in os.listdir(filefolderpath):\n",
    "            file_path = os.path.join(filefolderpath, file)\n",
    "            if os.path.isfile(file_path) and file.endswith(filetype):\n",
    "                filelist.append(file_path)\n",
    "\n",
    "    return filelist\n",
    "\n",
    "def read_combined_dataframe(file_list, filepath = True):\n",
    "    dataframes = []\n",
    "    \n",
    "    for file in file_list:\n",
    "        try:\n",
    "            if file.endswith('.csv'):\n",
    "                df = pd.read_csv(file)\n",
    "            elif file.endswith('.shp'):\n",
    "                df = gpd.read_file(file)\n",
    "            elif file.endswith(('.xls', '.xlsx')):\n",
    "                df = pd.read_excel(file)\n",
    "            else:\n",
    "                print(f\"Unsupported file format: {file}\")\n",
    "                continue\n",
    "            if filepath:\n",
    "                df['FilePath'] = file  # 添加來源檔案路徑欄位\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    # 合併所有 DataFrame\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "# 01_資料預處理\n",
    "def filter_ticket_data(filepath, \n",
    "                       selectdate_start, \n",
    "                       selectdate_end, \n",
    "                       outputfolder,\n",
    "                       skiprows=1, \n",
    "                       chunksize=1000,\n",
    "                        on_time_column = 'BoardingTime', \n",
    "                       off_time_column = 'DeboardingTime', \n",
    "                       infodate_column = 'InfoDate',):\n",
    "    \"\"\"\n",
    "    分批讀取大型票證 CSV，依上車時間欄位做日期篩選後輸出新的 CSV。\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        原始 CSV 路徑\n",
    "    on_time_column : str\n",
    "        上車時間欄位名稱\n",
    "    off_time_column : str\n",
    "        下車時間欄位名稱（保留未來擴充）\n",
    "    selectdate_start : str\n",
    "        篩選起始日期（YYYY-MM-DD）\n",
    "    selectdate_end : str\n",
    "        篩選結束日期（YYYY-MM-DD）\n",
    "    outputfolder : str\n",
    "        最終輸出 CSV 的資料夾路徑\n",
    "    skiprows : int\n",
    "        讀取 CSV 時跳過的列\n",
    "    chunksize : int\n",
    "        每批讀取筆數\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    outputpath : str\n",
    "        最終輸出 CSV 的完整路徑\n",
    "    \"\"\"\n",
    "\n",
    "    # 建立輸出資料夾（如不存在）\n",
    "    os.makedirs(outputfolder, exist_ok=True)\n",
    "\n",
    "    # 產生輸出檔名\n",
    "    filename = os.path.basename(filepath).replace(\n",
    "        \".csv\", f\"_{selectdate_start}_to_{selectdate_end}.csv\"\n",
    "    )\n",
    "    outputpath = os.path.join(outputfolder, filename)\n",
    "\n",
    "    # 日期轉 datetime\n",
    "    start = pd.to_datetime(selectdate_start)\n",
    "    end   = pd.to_datetime(selectdate_end)\n",
    "\n",
    "    # 分批讀取\n",
    "    chunks = pd.read_csv(filepath, skiprows=skiprows, chunksize=chunksize)\n",
    "    first_chunk = True\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # 轉成 datetime\n",
    "        # chunk[on_time_column] = pd.to_datetime(chunk[on_time_column], errors='coerce')\n",
    "        # chunk[off_time_column] = pd.to_datetime(chunk[off_time_column], errors='coerce')\n",
    "        chunk[infodate_column] = pd.to_datetime(chunk[infodate_column], errors='coerce')\n",
    "\n",
    "        # 日期篩選\n",
    "        # mask = (\n",
    "        #     ((chunk[on_time_column]  >= start) & (chunk[on_time_column]  <= end)) |\n",
    "        #     ((chunk[off_time_column] >= start) & (chunk[off_time_column] <= end))\n",
    "        # )    \n",
    "        # mask = (chunk[on_time_column] >= start) & (chunk[on_time_column] <= end)\n",
    "        mask = (chunk[infodate_column] >= start) & (chunk[infodate_column] <= end)\n",
    "        filtered_chunk = chunk[mask]\n",
    "\n",
    "        if filtered_chunk.empty:\n",
    "            continue\n",
    "\n",
    "        # 寫入 CSV\n",
    "        filtered_chunk.to_csv(\n",
    "            outputpath,\n",
    "            mode='w' if first_chunk else 'a',\n",
    "            header=first_chunk,\n",
    "            index=False,\n",
    "            encoding='utf-8-sig'\n",
    "        )\n",
    "        first_chunk = False\n",
    "\n",
    "    return outputpath\n",
    "\n",
    "def tickets_cleaning(\n",
    "    tickets,\n",
    "    on_time_column='BoardingTime',\n",
    "    off_time_column='DeboardingTime',\n",
    "    getonstop='BoardingStopUID',\n",
    "    getoffstop='DeboardingStopUID',\n",
    "    getonseq='BoardingStopSequence',\n",
    "    getoffseq='DeboardingStopSequence'):\n",
    "\n",
    "    n = len(tickets)\n",
    "\n",
    "    # ---- 型別轉換（你不把缺值當異常，但比較要正確）----\n",
    "    on_time  = pd.to_datetime(tickets[on_time_column], errors='coerce')\n",
    "    off_time = pd.to_datetime(tickets[off_time_column], errors='coerce')\n",
    "    on_seq   = pd.to_numeric(tickets[getonseq], errors='coerce')\n",
    "    off_seq  = pd.to_numeric(tickets[getoffseq], errors='coerce')\n",
    "    on_stop  = tickets[getonstop]\n",
    "    off_stop = tickets[getoffstop]\n",
    "\n",
    "    # ---- 能確定的三種異常（缺值不算異常）----\n",
    "    m_time_rev  = (on_time > off_time)               # 上車晚於下車\n",
    "    m_same_stop = (on_stop == off_stop)              # 同站上下車\n",
    "    # m_seq_err   = (on_seq >= off_seq)                # 上序 >= 下序\n",
    "\n",
    "    # ---- 資料正常（只有確定異常才算異常，其餘都正常）----\n",
    "    # m_ok = ~(m_time_rev | m_same_stop | m_seq_err)\n",
    "    m_ok = ~(m_time_rev | m_same_stop )\n",
    "\n",
    "    cleaned = tickets[m_ok].copy()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 依你的要求：新增 ErrorMsg 欄位，描述缺哪些資料（但不當異常）\n",
    "    # ---------------------------------------------------------\n",
    "    miss_off_time = off_time.isna()\n",
    "    miss_off_stop = off_stop.isna()\n",
    "\n",
    "    def combine_err(row):\n",
    "        msgs = []\n",
    "        if row['miss_off_time']:\n",
    "            msgs.append(\"沒有下車刷卡時間\")\n",
    "        if row['miss_off_stop']:\n",
    "            msgs.append(\"沒有下車站點資料\")\n",
    "        return \"；\".join(msgs)\n",
    "\n",
    "    temp_df = pd.DataFrame({\n",
    "        \"miss_off_time\": miss_off_time,\n",
    "        \"miss_off_stop\": miss_off_stop\n",
    "    })\n",
    "\n",
    "    cleaned[\"ErrorMsg\"] = temp_df.loc[cleaned.index].apply(combine_err, axis=1)\n",
    "    # 若沒有錯誤，改成空字串\n",
    "    cleaned[\"ErrorMsg\"] = cleaned[\"ErrorMsg\"].replace(\"\", \"\")\n",
    "\n",
    "    # ---- 統計輸出 ----\n",
    "    output = {\n",
    "        '原始票證數量': int(n),\n",
    "        '資料正常': int(m_ok.sum()),\n",
    "        '資料異常 - 上車晚於下車': int(m_time_rev.sum()),\n",
    "        '資料異常 - 同站上下車': int(m_same_stop.sum()),\n",
    "        # '資料異常 - 上下車次序錯誤': int(m_seq_err.sum()),\n",
    "        '資訊缺失 - 沒有下車刷卡時間': int(miss_off_time.sum()),\n",
    "        '資訊缺失 - 沒有下車站點資料': int(miss_off_stop.sum())\n",
    "    }\n",
    "\n",
    "    correctrate = round((output['資料正常'] / n) * 100, 2) if n else 0.0\n",
    "    return cleaned, output, correctrate\n",
    "\n",
    "def mark_ticket_errors(\n",
    "    tickets, \n",
    "    on_time_column='on_time_column', \n",
    "    off_time_column='off_time_column', \n",
    "    getonstop='GetOnStop', \n",
    "    getoffstop='GetOffStop', \n",
    "    getonseq='GetOnSeq', \n",
    "    getoffseq='GetOffSeq'):\n",
    "    \"\"\"\n",
    "    在票證資料上貼三種錯誤標籤，為 0/1。\n",
    "    不做篩選，不刪資料，只新增欄位。\n",
    "    \"\"\"\n",
    "    tickets['error_time'] = (tickets[on_time_column] > tickets[off_time_column]).astype(int)\n",
    "    tickets['error_same_stop'] = (tickets[getonstop] == tickets[getoffstop]).astype(int)\n",
    "    # tickets['error_seq'] = (tickets[getonseq] >= tickets[getoffseq]).astype(int)\n",
    "\n",
    "    # 判斷各欄是否為無效值（NaN、-99、\"-99\"）\n",
    "    tickets['error_onseq']  = (\n",
    "        tickets[getonseq].isin([-99, \"-99\"]) | tickets[getonseq].isna()\n",
    "    ).astype(int)\n",
    "\n",
    "    tickets['error_offseq'] = (\n",
    "        tickets[getoffseq].isin([-99, \"-99\"]) | tickets[getoffseq].isna()\n",
    "    ).astype(int)\n",
    "\n",
    "\n",
    "    tickets['error'] = (\n",
    "        (tickets['error_time'] == 1) |\n",
    "        (tickets['error_same_stop'] == 1) |\n",
    "        (tickets['error_onseq'] == 1) | \n",
    "        (tickets['error_offseq'] == 1)\n",
    "    ).astype(int)\n",
    "\n",
    "    return tickets\n",
    "\n",
    "def export_ticketcorrectrate(filename, output, correctrate, txt_path):\n",
    "\n",
    "    # 運算時間\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # 判斷檔案是否已存在\n",
    "    file_exists = os.path.exists(txt_path)\n",
    "\n",
    "    # 若檔案不存在 → 用 w (寫入 header)\n",
    "    # 若檔案存在 → 用 a (不寫 header)\n",
    "    mode = \"a\" if file_exists else \"w\"\n",
    "\n",
    "    with open(txt_path, mode, encoding=\"utf-8\") as f:\n",
    "\n",
    "        # 如果是新檔案，寫入 header\n",
    "        if not file_exists:\n",
    "            f.write(\"filename,timestamp,key,value\\n\")\n",
    "\n",
    "        # 寫入 output 每筆資料\n",
    "        for key, value in output.items():\n",
    "            f.write(f\"{filename},{timestamp},{key},{value}\\n\")\n",
    "\n",
    "        # 寫入正確率\n",
    "        f.write(f\"{filename},{timestamp},正確率,{correctrate}\\n\")\n",
    "\n",
    "    print(f\"TXT (CSV 格式) 已輸出：{txt_path}\")\n",
    "\n",
    "def get_stop_fromtickets(df):\n",
    "    \"\"\"\n",
    "    從票證資料中提取所有上下車站點資訊，並合併成一個包含所有站點的 DataFrame。\n",
    "    用於檢查票種的站點是否為可用的站點，因為有站點才有辦法核對到GIS。\n",
    "    \n",
    "    參數:\n",
    "    df (DataFrame): 包含票證資料的 DataFrame，需包含上下車站點相關欄位。\n",
    "    \n",
    "    回傳:\n",
    "    DataFrame: 包含所有上下車站點資訊的 DataFrame。\n",
    "    \"\"\"\n",
    "    \n",
    "     # 選取需要的欄位\n",
    "    select_columns = ['Authority', 'OperatorNo',  \n",
    "                    'RouteUID', 'RouteName', 'SubRouteUID', 'SubRouteName', 'Direction']\n",
    "    boarding_stop_columns = ['BoardingStopUID', 'BoardingStopName', 'BoardingStopSequence']\n",
    "    deboarding_stop_columns = ['DeboardingStopUID', 'DeboardingStopName', 'DeboardingStopSequence']\n",
    "\n",
    "    # 取上車資料\n",
    "    dfboarding =  df[select_columns + boarding_stop_columns]\n",
    "    dfboarding[select_columns + boarding_stop_columns] = dfboarding[select_columns + boarding_stop_columns].fillna('-99')\n",
    "    dfboarding.columns = dfboarding.columns.str.replace('Boarding', '')\n",
    "    dfboarding['OnorOff'] = 'On'\n",
    "\n",
    "    # 取下車資料\n",
    "    dfdeboarding =  df[select_columns + deboarding_stop_columns]\n",
    "    dfdeboarding[select_columns + deboarding_stop_columns] = dfdeboarding[select_columns+ deboarding_stop_columns].fillna('-99')\n",
    "    dfdeboarding.columns = dfdeboarding.columns.str.replace('Deboarding', '')\n",
    "    dfdeboarding['OnorOff'] = 'Off'\n",
    "    # 合併上下車站點資料\n",
    "    df_stops = pd.concat([dfboarding, dfdeboarding], ignore_index=True)\n",
    "    \n",
    "    df_stops = (\n",
    "        df_stops\n",
    "        .fillna(-99)\n",
    "        .groupby(df_stops.columns.tolist())\n",
    "        .size()\n",
    "        .reset_index(name='Count')\n",
    "    )\n",
    "\n",
    "    return df_stops\n",
    "\n",
    "def match_stop_coordinates(\n",
    "    dfstop, \n",
    "    stop_gdf, \n",
    "    col_uid=\"StopUID\", \n",
    "    col_name=\"StopName\", \n",
    "    col_lat=\"Lat\", \n",
    "    col_lon=\"Lon\"):\n",
    "    \"\"\"\n",
    "    進行兩階段站點比對，並將所有原本 print 的文字改成 text 文字回傳：\n",
    "    回傳：\n",
    "        dfcount_final : 二階段比對後結果 DataFrame\n",
    "        text : 報表文字（取代 print）\n",
    "    \"\"\"\n",
    "\n",
    "    text_output = []\n",
    "\n",
    "    # 第一次比對：比對 StopUID 與 StopName\n",
    "    dfcount = pd.merge(\n",
    "        dfstop,\n",
    "        stop_gdf[[col_uid, col_name, col_lon, col_lat]].drop_duplicates(subset=[col_uid, col_name]),\n",
    "        on=[col_uid, col_name],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    total = dfstop[\"Count\"].sum()\n",
    "    abnormal = dfcount[(dfcount[col_lon].isna()) | (dfcount[col_lat].isna())][\"Count\"].sum()\n",
    "\n",
    "    text_output.append(\"第一次比對結果\")\n",
    "    text_output.append(f\"總共有幾筆資料: {total:,}\")\n",
    "    text_output.append(f\"沒有對應經緯度座標的資料異常數量: {abnormal:,}\")\n",
    "    text_output.append(f\"影響比例: {abnormal / total:.4%}\")\n",
    "    text_output.append(\"============================\")\n",
    "\n",
    "    # 第二次比對：只比對 StopUID\n",
    "    dfcount_2ndround = dfcount[(dfcount[col_lon].isna()) | (dfcount[col_lat].isna())].copy()\n",
    "\n",
    "    dfcount_2ndround = pd.merge(\n",
    "        dfcount_2ndround.drop(columns=[col_lon, col_lat]),\n",
    "        stop_gdf[[col_uid, col_lon, col_lat, col_name]].drop_duplicates(subset=[col_uid]),\n",
    "        on=[col_uid],\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_gdf\")\n",
    "    )\n",
    "\n",
    "    total_2ndround = dfcount_2ndround[\"Count\"].sum()\n",
    "    abnormal_2ndround = dfcount_2ndround[(dfcount_2ndround[col_lon].isna()) | (dfcount_2ndround[col_lat].isna())][\"Count\"].sum()\n",
    "\n",
    "    text_output.append(\"第二次比對結果\")\n",
    "    text_output.append(f\"第二次比對 - 總共有幾筆資料: {total_2ndround:,}\")\n",
    "    text_output.append(f\"第二次比對 - 沒有對應經緯度座標的資料異常數量: {abnormal_2ndround:,}\")\n",
    "    text_output.append(f\"第二次比對 - 影響比例: {abnormal_2ndround / total_2ndround:.4%}\")\n",
    "    text_output.append(f\"第二次比對 - 影響佔可用票證的原始比例: {abnormal_2ndround / total:.4%}\")\n",
    "    text_output.append(\"============================\")\n",
    "\n",
    "    # 最終合併：第一次成功 + 第二次比對結果\n",
    "    dfcount_final = pd.concat(\n",
    "        [dfcount[~((dfcount[col_lon].isna()) | (dfcount[col_lat].isna()))], \n",
    "         dfcount_2ndround],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # 將文字合成一個字串\n",
    "    text = \"\\n\".join(text_output)\n",
    "\n",
    "    return dfcount_final, text\n",
    "\n",
    "# 預處理01: 指定時間區間票證資料切分\n",
    "def pre01_split_ticket_with_day(selectdate_start, selectdate_end, outputfolder):\n",
    "        orginal_ticket_files = [\n",
    "                                os.path.abspath(os.path.join(os.getcwd(), '..', '..', '2024_2025', '公路客運電子票證資料(TO1A)', '公路客運電子票證資料(TO1A).csv')), \n",
    "                                os.path.abspath(os.path.join(os.getcwd(), '..', '..', '2024_2025', '桃園市公車電子票證資料(TO1A)', '桃園市公車電子票證資料(TO1A).csv')), \n",
    "                                os.path.abspath(os.path.join(os.getcwd(), '..', '..', '2024_2025', '新北市公車電子票證資料(TO1A)', '新北市公車電子票證資料(TO1A).csv'))\n",
    "                                ]\n",
    "        for file in orginal_ticket_files:\n",
    "                output = filter_ticket_data(\n",
    "                        filepath = file,\n",
    "                        infodate_column = 'InfoDate',\n",
    "                        selectdate_start = selectdate_start,\n",
    "                        selectdate_end = selectdate_end,\n",
    "                        outputfolder = outputfolder,\n",
    "                        skiprows = 1,\n",
    "                        chunksize = 1000\n",
    "                        )\n",
    "                print(\"輸出路徑：\", output)\n",
    "\n",
    "# 預處理02: 過濾不合理票證資料(用站序資料\n",
    "def pre02_get_correct_tickets(selecttime_ticket_folder, checkok_ticketfolder):\n",
    "\n",
    "    selecttime_ticket_files = findfiles(selecttime_ticket_folder, filetype='.csv', recursive=False)\n",
    "    correctratelog_path = os.path.join(checkok_ticketfolder, '客運票證資料正確率記錄.txt')\n",
    "\n",
    "    chunksize = 10000   \n",
    "\n",
    "    for file in selecttime_ticket_files:\n",
    "\n",
    "        print(f\"\\n=== 開始處理：{file} ===\")\n",
    "\n",
    "        # 統計資料累加器\n",
    "        total_stat = Counter()\n",
    "\n",
    "        # 輸出清洗後 CSV 的路徑\n",
    "        cleaned_output_path = os.path.join(\n",
    "            checkok_ticketfolder,\n",
    "            os.path.basename(file).replace(\".csv\", \"_cleaned.csv\")\n",
    "        )\n",
    "\n",
    "        first_chunk = True  # 控制 header\n",
    "\n",
    "        # 分批讀取整個檔案\n",
    "        for chunk in pd.read_csv(file, chunksize=chunksize, encoding='utf-8-sig'):\n",
    "\n",
    "            # 跑你自己的清洗函數\n",
    "            cleaned_df, correct_stat_info, correctrate_chunk = tickets_cleaning(\n",
    "                chunk,\n",
    "                on_time_column='BoardingTime',\n",
    "                off_time_column='DeboardingTime',\n",
    "                getonstop='BoardingStopUID',\n",
    "                getoffstop='DeboardingStopUID',\n",
    "                getonseq='BoardingStopSequence',\n",
    "                getoffseq='DeboardingStopSequence'\n",
    "            )\n",
    "\n",
    "            # 累加統計\n",
    "            total_stat.update(correct_stat_info)\n",
    "\n",
    "            # 將清洗後的 cleaned_df 分批寫入新 CSV\n",
    "            if not cleaned_df.empty:\n",
    "                cleaned_df.to_csv(\n",
    "                    cleaned_output_path,\n",
    "                    mode='w' if first_chunk else 'a',\n",
    "                    header=first_chunk,\n",
    "                    index=False,\n",
    "                    encoding='utf-8-sig'\n",
    "                )\n",
    "                first_chunk = False\n",
    "\n",
    "        # -------- 整份 CSV 的整體正確率 --------\n",
    "        original_count = total_stat.get('原始票證數量', 0)\n",
    "        canuse_count   = total_stat.get('資料正常', 0)\n",
    "\n",
    "        if original_count > 0:\n",
    "            final_correctrate = round(canuse_count / original_count * 100, 2)\n",
    "        else:\n",
    "            final_correctrate = 0.0\n",
    "\n",
    "        # -------- 寫入 TXT（CSV 格式） --------\n",
    "        export_ticketcorrectrate(\n",
    "            filename=file,\n",
    "            output=dict(total_stat),\n",
    "            correctrate=final_correctrate,\n",
    "            txt_path=correctratelog_path\n",
    "        )\n",
    "\n",
    "        print(f\"清洗後資料輸出：{cleaned_output_path}\")\n",
    "\n",
    "# 預處理03: 確認所有站點的經緯度在TDX都可以被核對出來\n",
    "def pre03_findstops(checkok_ticketfolder, \n",
    "                    seqfolder = r\"D:\\B-Project\\2025\\6800\\Technical\\12票證資料\\TicketAnalysis\\00_TDX資料下載\\01公車站序資料\"):\n",
    "\n",
    "    files = findfiles(checkok_ticketfolder)\n",
    "    files = [f for f in files if 'TO1' in f]\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file, encoding='utf-8-sig')\n",
    "        stop = get_stop_fromtickets(df)\n",
    "        stop['file_source'] = os.path.basename(file)\n",
    "\n",
    "        outputfilename = os.path.join(check_stopfolder, os.path.basename(file).replace('_cleaned.csv', '_stops.csv'))\n",
    "        stop.to_csv(outputfilename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"站點資料輸出：{outputfilename}\")\n",
    "\n",
    "    df_stop = read_combined_dataframe(findfiles(check_stopfolder, filetype='csv', recursive=False), filepath=False)\n",
    "\n",
    "    df_seq = read_combined_dataframe(findfiles(seqfolder, \n",
    "                                            filetype='csv', \n",
    "                                            recursive=False), filepath=False)\n",
    "    df_stopfromseq = df_seq[['StopUID', 'StopName_Zh', 'PositionLon', 'PositionLat']].drop_duplicates(subset=['StopUID']).sort_values(['StopUID'])\n",
    "\n",
    "    df_final, report_text = match_stop_coordinates(\n",
    "        dfstop=df_stop.copy().rename(columns = {'StopName':'StopName_Zh'}),\n",
    "        stop_gdf=df_stopfromseq,\n",
    "        col_uid=\"StopUID\",\n",
    "        col_name=\"StopName_Zh\",\n",
    "        col_lat=\"PositionLat\",\n",
    "        col_lon=\"PositionLon\"\n",
    "    )\n",
    "\n",
    "    print(report_text)\n",
    "\n",
    "\n",
    "\n",
    "    # a = df_final[((df_final['PositionLon'].isna()) | (df_final['PositionLat'].isna())) & (df_final['StopUID'] != \"-99\")][['StopUID', 'StopName_Zh']].drop_duplicates()\n",
    "    # a['Auth'] = a['StopUID'].str[:3]\n",
    "    # a.sort_values(['Auth'])\n",
    "\n",
    "# 預處理04: 加上必要欄位 (平假日欄位、刪除不重要的欄位）\n",
    "def add_weekdayandweekendcolumns(df, \n",
    "                                 timecolumns='InfoDate',\n",
    "                                 filterdate=None):\n",
    "    \"\"\"\n",
    "    將 DataFrame 中的時間欄位轉換為日期時間格式，新增 DaysofWeek 和 WDWK 欄位，\n",
    "    並可選擇性地過濾掉特定日期。\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): 原始 DataFrame。\n",
    "        timecolumns (str): 包含日期的欄位名稱，預設為 'InfoDate'。\n",
    "        filterdate (list/None): 要過濾掉的日期字串列表 (例如 ['YYYY-MM-DD'])。\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 處理後的 DataFrame。\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. 將時間欄位轉換為 datetime\n",
    "    df[timecolumns] = pd.to_datetime(df[timecolumns], errors='coerce')\n",
    "\n",
    "    # 2. 新增 'DaysofWeek' 欄位\n",
    "    \n",
    "    df['DaysofWeek'] = df[timecolumns].dt.dayofweek # .dt.dayofweek 會回傳：0=週一, 1=週二, ..., 6=週日\n",
    "\n",
    "    # 3. 處理過濾日期 (如果 filterdate 不是 None 且有內容)\n",
    "    if filterdate and len(filterdate) > 0:\n",
    "        # 將 filterdate 列表轉換為 datetime 格式，以便進行比較\n",
    "        filter_dates_dt = pd.to_datetime(filterdate)\n",
    "        \n",
    "        # 找出不在 filter_dates_dt 中的日期 (布林遮罩)\n",
    "        # .dt.normalize() 將日期時間的時間部分設為 00:00:00，確保只比較日期\n",
    "        filter_mask = ~df[timecolumns].dt.normalize().isin(filter_dates_dt)\n",
    "        \n",
    "        # 套用遮罩，只保留不在過濾列表中的資料\n",
    "        df = df[filter_mask].copy()\n",
    "\n",
    "    # 4. 新增 'WDWK' 欄位\n",
    "    # .dt.dayofweek 回傳：0=週一, 1=週二, 2=週三, 3=週四, 4=週五, 5=週六, 6=週日\n",
    "    \n",
    "    # 定義條件：\n",
    "    # WDWK = 1 (週二=1, 週三=2, 週四=3)\n",
    "    wdwk_1_condition = df['DaysofWeek'].isin([1, 2, 3])\n",
    "    \n",
    "    # WDWK = -1 (週六=5, 週日=6)\n",
    "    wdwk_neg1_condition = df['DaysofWeek'].isin([5, 6])\n",
    "    \n",
    "    # 使用 np.select (比多個 if/elif 判斷更快)\n",
    "    \n",
    "    df['WDWK'] = np.select(\n",
    "        [wdwk_1_condition, wdwk_neg1_condition], # 條件列表\n",
    "        [1, 0],                                # 對應的值\n",
    "        default=-1                               # 預設值 (其他日子=1)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "def must_outputformat(df):\n",
    "    df['BoardingTime'] = pd.to_datetime(df['BoardingTime'], errors='coerce')\n",
    "    df['DeboardingTime'] = pd.to_datetime(df['DeboardingTime'], errors='coerce')\n",
    "    df['BoardinngDate'] = df['BoardingTime'].dt.date\n",
    "    df['DeboardingDate'] = df['DeboardingTime'].dt.date\n",
    "    df['BoardingHour'] = df['BoardingTime'].dt.hour\n",
    "    df['DeboardingHour'] = df['DeboardingTime'].dt.hour\n",
    "\n",
    "    reindexcolumns = ['Authority', 'OperatorNo', 'HolderType', 'TicketType', 'SubTicketType', \n",
    "                    'RouteUID', 'RouteName', 'SubRouteUID', 'SubRouteName', 'Direction', \n",
    "                    'BoardingStopUID', 'BoardingStopName', 'BoardingStopSequence', 'BoardinngDate',  'BoardingHour', \n",
    "                    'DeboardingStopUID', 'DeboardingStopName', 'DeboardingStopSequence', 'DeboardingDate', 'DeboardingHour',\n",
    "                    'InfoDate', 'DaysofWeek', 'WDWK']\n",
    "\n",
    "    df = df.reindex(columns=reindexcolumns)\n",
    "    return df \n",
    "def pre04_reformat(checkok_ticketfolder, reformat_folder, filterdate = None):\n",
    "\n",
    "    filelist = findfiles(checkok_ticketfolder, filetype='csv', recursive=False)\n",
    "\n",
    "    for file in filelist:\n",
    "\n",
    "        reformat_output_file = os.path.join(\n",
    "            reformat_folder,\n",
    "            os.path.basename(file).replace(\"_cleaned.csv\", \"_reformatted.csv\")\n",
    "        )\n",
    "\n",
    "\n",
    "        # 如果 mark_ticket_errors 需要全表上下文，改成 chunksize=None\n",
    "        reader = pd.read_csv(file, chunksize=1000)\n",
    "\n",
    "        first_chunk = True\n",
    "        for chunk in reader:\n",
    "\n",
    "            output = add_weekdayandweekendcolumns(df=chunk,\n",
    "                                            timecolumns= 'InfoDate', \n",
    "                                            filterdate = filterdate)\n",
    "            output = must_outputformat(output)\n",
    "\n",
    "            output.to_csv(\n",
    "                reformat_output_file,\n",
    "                mode='w' if first_chunk else 'a',\n",
    "                header=first_chunk,\n",
    "                index=False,\n",
    "                encoding='utf-8-sig'\n",
    "            )\n",
    "            first_chunk = False  \n",
    "\n",
    "# 預處理05: 重新比對站序\n",
    "def checkseq(df, df_seq, process_step = 2):\n",
    "\n",
    "    df = df.copy()\n",
    "    df_seq = df_seq.copy()\n",
    "\n",
    "    df = df[~((df['BoardingStopUID'] == \"-99\") | (df['DeboardingStopUID'] == \"-99\"))]\n",
    "\n",
    "    df_seq = df_seq.reindex(columns = ['RouteUID', 'SubRouteUID', 'Direction', 'StopUID', 'StopName_Zh', 'StopSequence']).rename(columns = {'StopName_Zh':'StopName'})\n",
    "\n",
    "    df = pd.merge(df, \n",
    "                df_seq.rename(columns = {'StopUID':'BoardingStopUID',\n",
    "                                        'StopName':'BoardingStopName_S', \n",
    "                                        'StopSequence':'BoardingStopSequence_S'}).drop_duplicates(subset = ['RouteUID', 'SubRouteUID', 'Direction', 'BoardingStopUID']),\n",
    "                on = ['RouteUID', 'SubRouteUID', 'Direction', 'BoardingStopUID'],\n",
    "                how = 'left')\n",
    "\n",
    "    df = pd.merge(df, \n",
    "                df_seq.rename(columns = {'StopUID':'DeboardingStopUID',\n",
    "                                        'StopName':'DeboardingStopName_S', \n",
    "                                        'StopSequence':'DeboardingStopSequence_S'}).drop_duplicates(subset = ['RouteUID', 'SubRouteUID', 'Direction', 'DeboardingStopUID']),\n",
    "                on = ['RouteUID', 'SubRouteUID', 'Direction', 'DeboardingStopUID'],\n",
    "                how = 'left')\n",
    "\n",
    "    df_keep = df[(df['BoardingStopSequence'] == df['BoardingStopSequence_S']) & \n",
    "                (df['DeboardingStopSequence'] == df['DeboardingStopSequence_S'])].drop(columns = ['BoardingStopName_S', 'BoardingStopSequence_S', 'DeboardingStopName_S', 'DeboardingStopSequence_S'])\n",
    "\n",
    "    df_process = df[(df['BoardingStopSequence'] != df['BoardingStopSequence_S']) |\n",
    "                    (df['DeboardingStopSequence'] != df['DeboardingStopSequence_S'])]\n",
    "\n",
    "    \n",
    "    \n",
    "    if process_step == 2:\n",
    "\n",
    "        # 轉成數值，轉換失敗為 NaN\n",
    "        df_process['BoardingStopSequence_S'] = pd.to_numeric(df_process['BoardingStopSequence_S'], errors='coerce')\n",
    "        df_process['DeboardingStopSequence_S'] = pd.to_numeric(df_process['DeboardingStopSequence_S'], errors='coerce')\n",
    "\n",
    "        # 建立布林遮罩\n",
    "        mask = df_process['BoardingStopSequence_S'] < df_process['DeboardingStopSequence_S']\n",
    "        mask = mask.fillna(False)   # 避免 NaN 導致問題（可選）\n",
    "\n",
    "        # 依條件寫入主欄位\n",
    "        df_process.loc[mask, 'BoardingStopSequence']  = df_process.loc[mask, 'BoardingStopSequence_S']\n",
    "        df_process.loc[mask, 'DeboardingStopSequence'] = df_process.loc[mask, 'DeboardingStopSequence_S']\n",
    "\n",
    "        # 要刪掉的暫存欄位\n",
    "        cols_to_drop = ['BoardingStopName_S', 'BoardingStopSequence_S',\n",
    "                        'DeboardingStopName_S', 'DeboardingStopSequence_S']\n",
    "\n",
    "        # concat 前先 drop\n",
    "        df_keep = pd.concat([\n",
    "            df_keep,\n",
    "            df_process.loc[mask].drop(columns=cols_to_drop, errors='ignore')\n",
    "        ], ignore_index=True)\n",
    "\n",
    "        # 剩下的資料繼續處理\n",
    "        df_process = df_process.loc[~mask].copy()\n",
    "    \n",
    "    return df_keep, df_process\n",
    "def matchwithstopname(df, df_seq, df_done):\n",
    "    df_seq = df_seq.copy()\n",
    "    df = df.copy()\n",
    "    df = df[(df['BoardingStopName'] != df['DeboardingStopName'] )]\n",
    "\n",
    "    reindexcolumns = df.columns.tolist()\n",
    "    donecolumns = df_done.head().columns.tolist()\n",
    "    df_seq = df_seq.reindex(columns = ['RouteUID', 'SubRouteUID', 'Direction', 'StopUID', 'StopName_Zh', 'StopSequence']).rename(columns = {'StopName_Zh':'StopName'})\n",
    "\n",
    "\n",
    "    # 一、用RouteUID、SubRouteUID、StopName的組合再比對一次站序及站點編號 (因比對不到的)\n",
    "    # 1. 比對起點站名\n",
    "    mask_boardingseq = df['BoardingStopSequence_S'].isna()\n",
    "    df_temp = df[mask_boardingseq].copy()\n",
    "    df_temp = df_temp.drop(columns = ['BoardingStopName_S', 'BoardingStopSequence_S'])\n",
    "\n",
    "    df_temp = pd.merge(df_temp, \n",
    "                    df_seq.rename(columns = {'StopUID':'BoardingStopUID_S',\n",
    "                                                'StopName':'BoardingStopName', \n",
    "                                                'StopSequence':'BoardingStopSequence_S'}).drop_duplicates(subset = ['RouteUID', 'SubRouteUID', 'Direction', 'BoardingStopName']),\n",
    "                    on = ['RouteUID', 'SubRouteUID', 'Direction', 'BoardingStopName'],\n",
    "                    how = 'left')\n",
    "\n",
    "    mask_boardingstopuid = df_temp['BoardingStopSequence_S'].notna()\n",
    "    df_temp.loc[mask_boardingstopuid, 'BoardingStopSequence'] = df_temp.loc[mask_boardingstopuid, 'BoardingStopSequence_S']\n",
    "    df_temp.loc[mask_boardingstopuid, 'BoardingStopUID'] = df_temp.loc[mask_boardingstopuid, 'BoardingStopUID_S']\n",
    "    df = pd.concat([df[~mask_boardingseq], \n",
    "                    df_temp]).sort_index()\n",
    "\n",
    "    df = df.reindex(columns=reindexcolumns)\n",
    "\n",
    "    # 2. 比對起點站名\n",
    "    mask_deboardingseq = df['DeboardingStopSequence_S'].isna()\n",
    "    df_temp = df[mask_deboardingseq].copy()\n",
    "    df_temp = df_temp.drop(columns = ['DeboardingStopName_S', 'DeboardingStopSequence_S'])\n",
    "\n",
    "    df_temp = pd.merge(df_temp, \n",
    "                       df_seq.rename(columns = {'StopUID':'DeboardingStopUID_S',\n",
    "                                                'StopName':'DeboardingStopName', \n",
    "                                                'StopSequence':'DeboardingStopSequence_S'}).drop_duplicates(subset = ['RouteUID', 'SubRouteUID', 'Direction', 'DeboardingStopName']),\n",
    "                       on = ['RouteUID', 'SubRouteUID', 'Direction', 'DeboardingStopName'],\n",
    "                       how = 'left')\n",
    "\n",
    "    mask_deboardingstopuid = df_temp['DeboardingStopSequence_S'].notna()\n",
    "    df_temp.loc[mask_deboardingstopuid, 'DeboardingStopSequence'] = df_temp.loc[mask_deboardingstopuid, 'DeboardingStopSequence_S']\n",
    "    df_temp.loc[mask_deboardingstopuid, 'DeboardingStopUID'] = df_temp.loc[mask_deboardingstopuid, 'DeboardingStopUID_S']\n",
    "    df = pd.concat([df[~mask_deboardingseq], \n",
    "                    df_temp]).sort_index()\n",
    "\n",
    "    df = df.reindex(columns=reindexcolumns)\n",
    "\n",
    "    # 二、更新他們的StopSequence\n",
    "    mask = (df['BoardingStopSequence_S'] < df['DeboardingStopSequence_S'])\n",
    "    df.loc[mask, 'BoardingStopSequence'] = df.loc[mask, 'BoardingStopSequence_S']\n",
    "    df.loc[mask, 'DeboardingStopSequence'] = df.loc[mask, 'DeboardingStopSequence_S']\n",
    "\n",
    "    df_done = pd.concat([df_done, \n",
    "                         df[mask]])\n",
    "    df_done = df_done.reindex(columns = donecolumns)\n",
    "    \n",
    "    \n",
    "\n",
    "    return df_done, df[~mask]\n",
    "def matchwith_anotherdirection(df, df_seq, df_done):\n",
    "\n",
    "    # 轉方向再計算一次\n",
    "    done_columns = df_done.columns.tolist()\n",
    "    \n",
    "    df_seq = df_seq.reindex(columns = ['RouteUID', 'SubRouteUID', 'Direction', 'StopUID', 'StopName_Zh', 'StopSequence']).rename(columns = {'StopName_Zh':'StopName'})\n",
    "\n",
    "    mask = df['BoardingStopSequence_S'] > df['DeboardingStopSequence_S']\n",
    "    df_temp = df[mask]\n",
    "\n",
    "    df_temp['Direction_another'] = 1 - df_temp['Direction']\n",
    "    df_temp = pd.merge(df_temp, \n",
    "                    df_seq.rename(columns = {'StopUID':'BoardingStopUID',\n",
    "                                                'StopName':'BoardingStopName_S2', \n",
    "                                                'StopSequence':'BoardingStopSequence_S2',\n",
    "                                                'Direction':'Direction_another'}).drop_duplicates(subset = ['RouteUID', 'SubRouteUID', 'Direction_another', 'BoardingStopUID']),\n",
    "                        on = ['RouteUID', 'SubRouteUID', 'Direction_another', 'BoardingStopUID'],\n",
    "                        how = 'left')\n",
    "\n",
    "    df_temp = pd.merge(df_temp, \n",
    "                    df_seq.rename(columns = {'StopUID':'DeboardingStopUID',\n",
    "                                                'StopName':'DeboardingStopName_S2', \n",
    "                                                'StopSequence':'DeboardingStopSequence_S2',\n",
    "                                                'Direction':'Direction_another'}).drop_duplicates(subset = ['RouteUID', 'SubRouteUID', 'Direction_another', 'DeboardingStopUID']),\n",
    "                        on = ['RouteUID', 'SubRouteUID', 'Direction_another', 'DeboardingStopUID'],\n",
    "                        how = 'left')\n",
    "\n",
    "    mask_anotherdirection = df_temp['BoardingStopSequence_S2'] < df_temp['DeboardingStopSequence_S2']\n",
    "    df_temp.loc[mask_anotherdirection, 'Direction'] = df_temp.loc[mask_anotherdirection, 'Direction_another']\n",
    "    df_temp.loc[mask_anotherdirection, 'BoardingStopSequence'] = df_temp.loc[mask_anotherdirection, 'BoardingStopSequence_S2']\n",
    "    df_temp.loc[mask_anotherdirection, 'DeboardingStopSequence'] = df_temp.loc[mask_anotherdirection, 'DeboardingStopSequence_S2']\n",
    "    df_done = pd.concat([df_done, \n",
    "                        df_temp[mask_anotherdirection].reindex(columns = done_columns)])\n",
    "\n",
    "    df_temp = df_temp[~mask_anotherdirection]\n",
    "\n",
    "    mask_turnaround = ((df_temp['BoardingStopSequence_S2'].notna()) & (df_temp['DeboardingStopSequence_S2'].isna())) & (df_temp['BoardingStopSequence_S2'] < df_temp['DeboardingStopSequence_S'])\n",
    "    df_temp.loc[mask_turnaround, 'BoardingStopSequence'] = df_temp.loc[mask_turnaround, 'BoardingStopSequence_S2']\n",
    "    df_temp.loc[mask_turnaround, 'DeboardingStopSequence'] = df_temp.loc[mask_turnaround, 'DeboardingStopSequence_S']\n",
    "    df_temp.loc[mask_turnaround, 'Direction'] = df_temp.loc[mask_turnaround, 'Direction_another']\n",
    "    df_done = pd.concat([df_done, \n",
    "                        df_temp[mask_turnaround].reindex(columns = done_columns)])\n",
    "\n",
    "    df_temp = df_temp[~mask_turnaround]\n",
    "    df_temp = pd.concat([df[~mask], \n",
    "                         df_temp])\n",
    "\n",
    "    return df_done, df_temp\n",
    "def pre05_redefined_stopsequence(outputfolder):\n",
    "    # 讀取預處理資料 \n",
    "    df = pd.read_csv( os.path.abspath(os.path.join(os.getcwd(), '..', '..', 'TicketAnalysis', '02_初步分析', '01_分時計次', '上下車區分票種分時計次(未修正站序是否正確).csv')))\n",
    "    print(len(df))\n",
    "\n",
    "    seqfolder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', 'TicketAnalysis', '00_TDX資料下載', '01公車站序資料'))\n",
    "    df_seq = read_combined_dataframe(file_list=findfiles(seqfolder))\n",
    "    df_seq = df_seq.reindex(columns = ['RouteUID', 'RouteName_Zh', 'SubRouteUID', 'SubRouteName_Zh', 'Direction', 'StopUID', 'StopName_Zh', 'StopSequence', 'PositionLon', 'PositionLat'])\n",
    "\n",
    "    df_done, df_temp = checkseq(df, df_seq) # 根據RouteUID、SubRouteUID、Direction、StopUID 比對出新的站序\n",
    "    print(\"第1次處理:透過StopUID比對\")\n",
    "    print(len(df_done), len(df_temp), len(df_done) + len(df_temp))\n",
    "\n",
    "    df_done, df_temp = matchwithstopname(df = df_temp, df_seq = df_seq, df_done = df_done) # 根據RouteUID、SubRouteUID、Direction、StopName 比對出新的站序\n",
    "    print(\"第2次處理:透過StopName比對\")\n",
    "    print(len(df_done), len(df_temp), len(df_done) + len(df_temp))\n",
    "\n",
    "    df_done, df_temp = matchwith_anotherdirection(df = df_temp, df_seq = df_seq, df_done = df_done) # 比較另外一個方向的站序，如果是通往底站折返的站點轉換為另外一個方向的站點資訊\n",
    "    print(\"第3次處理：轉方向處理\")\n",
    "    print(len(df_done), len(df_temp), len(df_done) + len(df_temp))\n",
    "\n",
    "    outputfilepath = os.path.join(outputfolder, '上下車區分票種分時計次.csv')\n",
    "    # df_done.to_csv(outputfilepath, index=False, encoding='utf-8-sig')\n",
    "    return df_done, df_temp    \n",
    "\n",
    "# 額外處理 -> 為了找到是否有問題的\n",
    "def extension_mark(selecttime_ticket_folder, checkok_ticketfolder):\n",
    "    marked_ticketfolder = create_folder(\n",
    "        os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '01-01_指定時間區間票證資料_但有錯誤標記')\n",
    "    )\n",
    "\n",
    "    selecttime_ticket_files = findfiles(selecttime_ticket_folder, filetype='.csv', recursive=False)\n",
    "    selecttime_ticket_files = [f for f in selecttime_ticket_files if 'TO1' in f]\n",
    "\n",
    "    for file in selecttime_ticket_files:\n",
    "        marked_output_file = os.path.join(\n",
    "            marked_ticketfolder,\n",
    "            os.path.basename(file).replace(\".csv\", \"_marked.csv\")\n",
    "        )\n",
    "\n",
    "        cleaned_output_file = os.path.join(\n",
    "            checkok_ticketfolder,\n",
    "            os.path.basename(file).replace(\".csv\", \"_cleaned.csv\")\n",
    "        )\n",
    "\n",
    "        # 如果 mark_ticket_errors 需要全表上下文，改成 chunksize=None\n",
    "        reader = pd.read_csv(file, chunksize=1000)\n",
    "\n",
    "        first_chunk = True\n",
    "        for chunk in reader:\n",
    "            output = mark_ticket_errors(\n",
    "                tickets=chunk, \n",
    "                on_time_column='BoardingTime',\n",
    "                off_time_column='DeboardingTime',\n",
    "                getonstop='BoardingStopUID',\n",
    "                getoffstop='DeboardingStopUID',\n",
    "                getonseq='BoardingStopSequence',\n",
    "                getoffseq='DeboardingStopSequence'\n",
    "            )\n",
    "\n",
    "            output.to_csv(\n",
    "                marked_output_file,\n",
    "                mode='w' if first_chunk else 'a',\n",
    "                header=first_chunk,\n",
    "                index=False,\n",
    "                encoding='utf-8-sig'\n",
    "            )\n",
    "\n",
    "            output[output['error'] != 1].drop(columns = ['error_time', 'error_same_stop', 'error_onseq', 'error_offseq', 'error']).to_csv(\n",
    "                cleaned_output_file,\n",
    "                mode='w' if first_chunk else 'a',\n",
    "                header=first_chunk,\n",
    "                index=False,\n",
    "                encoding='utf-8-sig'\n",
    "            )\n",
    "\n",
    "            first_chunk = False  \n",
    "\n",
    "# 02_資料分析處理\n",
    "\n",
    "# 分析01: 確認資料各票種、各路線、平假日、起點、迄點筆數\n",
    "def analytics01_hourlycount(reformat_folder, \n",
    "                            hourlycount_folder, \n",
    "                            seqfolder = r\"D:\\B-Project\\2025\\6800\\Technical\\12票證資料\\TicketAnalysis\\00_TDX資料下載\\01公車站序資料\",\n",
    "                            returndf = True):\n",
    "\n",
    "    files = findfiles(reformat_folder)\n",
    "    files = [f for f in files if 'TO1' in f]\n",
    "    df = read_combined_dataframe(files)\n",
    "\n",
    "    groupbycolumns = ['InfoDate', 'DaysofWeek', 'WDWK','Authority', 'HolderType', \n",
    "                      'RouteUID', 'RouteName', 'SubRouteUID', 'SubRouteName', 'Direction',\n",
    "                      'BoardingStopUID', 'BoardingStopName', 'BoardingStopSequence', 'BoardinngDate', 'BoardingHour',\n",
    "                      'DeboardingStopUID', 'DeboardingStopName', 'DeboardingStopSequence', 'DeboardingDate', 'DeboardingHour', 'FilePath']\n",
    "\n",
    "    df[groupbycolumns] = df[groupbycolumns].fillna('-99')\n",
    "    df_count = df.groupby(groupbycolumns).size().reset_index(name='Count')\n",
    "\n",
    "    df_seq = read_combined_dataframe(findfiles(seqfolder, \n",
    "                                            filetype='csv', \n",
    "                                            recursive=False), filepath=False)\n",
    "    df_stopfromseq = df_seq[['StopUID', 'StopName_Zh', 'PositionLon', 'PositionLat']].drop_duplicates(subset=['StopUID']).sort_values(['StopUID'])\n",
    "\n",
    "    df_count = pd.merge(df_count, \n",
    "                        df_stopfromseq[['StopUID', 'PositionLon', 'PositionLat']].rename(columns = {'StopUID':'BoardingStopUID', 'PositionLon':'BoardingLon', 'PositionLat':'BoardingLat'}), \n",
    "                        on = 'BoardingStopUID', \n",
    "                        how='left')\n",
    "\n",
    "    df_count = pd.merge(df_count, \n",
    "                        df_stopfromseq[['StopUID', 'PositionLon', 'PositionLat']].rename(columns = {'StopUID':'DeboardingStopUID', 'PositionLon':'DeboardingLon', 'PositionLat':'DeboardingLat'}), \n",
    "                        on = 'DeboardingStopUID', \n",
    "                        how='left')\n",
    "    df_count = df_count.reindex(columns= ['InfoDate', 'DaysofWeek', 'WDWK', 'HolderType', 'RouteUID', 'RouteName', 'SubRouteUID', 'SubRouteName', 'Direction',\n",
    "                                        'BoardingStopUID', 'BoardingStopName', 'BoardingStopSequence','BoardinngDate', 'BoardingHour', 'BoardingLon', 'BoardingLat', \n",
    "                                        'DeboardingStopUID','DeboardingStopName', 'DeboardingStopSequence', 'DeboardingDate', 'DeboardingHour', 'DeboardingLon', 'DeboardingLat', \n",
    "                                        'FilePath', 'Count'])\n",
    "\n",
    "    outputfile = os.path.join(hourlycount_folder, '上下車區分票種分時計次(未修正站序是否正確).csv')\n",
    "    df_count.to_csv(outputfile, index=False)\n",
    "\n",
    "    if returndf:\n",
    "        return df_count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1428724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00_Setup 所有全域函數\n",
    "\n",
    "# 1.) 設定篩選日期區間\n",
    "selectdate_start = '2024-10-01'\n",
    "selectdate_end = '2024-11-30'\n",
    "\n",
    "# 2.) 資料input資料夾\n",
    "referencefolder = os.path.abspath(os.path.join(os.getcwd(), '..', '參考資料'))\n",
    "\n",
    "# 3.) 建立輸出資料夾\n",
    "selecttime_ticket_folder = create_folder(os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '01_指定時間區間票證資料')) # 建立01-01 指定時間區間票證資料夾\n",
    "checkok_ticketfolder = create_folder(os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '02_過濾可用票證資料')) # 建立01-02 過濾可用票證資料夾\n",
    "check_stopfolder = create_folder(os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '03_所有使用到的點位')) # 建立01-03 所有使用到的點位資料夾\n",
    "reformat_folder = create_folder(os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '04_計算交通量格式')) # 建立01-03 所有使用到的點位資料夾\n",
    "correctseq_folder = create_folder(os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '05_重新處理票證站序')) # 建立01-03 所有使用到的點位資料夾\n",
    "\n",
    "hourlycount_folder = create_folder(os.path.join(os.getcwd(), '..', '02_初步分析', '01_分時計次')) # 建立01-03 所有使用到的點位資料夾\n",
    "dailybetweenstops_folder = create_folder(os.path.join(os.getcwd(), '..', '02_初步分析', '02_全日站間量'))\n",
    "od_folder = create_folder(os.path.join(os.getcwd(), '..', '02_初步分析', '03_OD起迄量'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6e8a510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\B-Project\\\\2025\\\\6800\\\\Technical\\\\12票證資料\\\\TicketAnalysis\\\\參考資料\\\\Date.xlsx'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(referencefolder, 'Date.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3ec7599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(os.path.abspath(os.path.join(referencefolder, 'Date.xlsx')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b02f38d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (685424331.py, line 44)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdf, dftemp = pre05_redefined_stopsequence(outputfolder = hourlycount_folder)\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# def main ():\n",
    "\n",
    "#     '''Setup 設定所有全域函數'''\n",
    "#     # 1.) 設定篩選日期區間\n",
    "#     selectdate_start = '2024-10-01'\n",
    "#     selectdate_end = '2024-11-30'\n",
    "\n",
    "#     # 2.) 資料input資料夾\n",
    "\n",
    "#     # 3.) 建立輸出資料夾\n",
    "#     selecttime_ticket_folder = create_folder(os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '01_指定時間區間票證資料')) # 建立01-01 指定時間區間票證資料夾\n",
    "#     checkok_ticketfolder = create_folder(os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '02_過濾可用票證資料')) # 建立01-02 過濾可用票證資料夾\n",
    "#     check_stopfolder = create_folder(os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '03_所有使用到的點位')) # 建立01-03 所有使用到的點位資料夾\n",
    "#     reformat_folder = create_folder(os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '04_計算交通量格式')) # 建立01-03 所有使用到的點位資料夾\n",
    "#     correctseq_folder = create_folder(os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '05_重新處理票證站序')) # 建立01-03 所有使用到的點位資料夾\n",
    "\n",
    "#     hourlycount_folder = create_folder(os.path.join(os.getcwd(), '..', '02_初步分析', '01_分時計次')) # 建立01-03 所有使用到的點位資料夾\n",
    "#     dailybetweenstops_folder = create_folder(os.path.join(os.getcwd(), '..', '02_初步分析', '02_全日站間量'))\n",
    "#     od_folder = create_folder(os.path.join(os.getcwd(), '..', '02_初步分析', '03_OD起迄量'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     '''資料處理步驟'''\n",
    "#     # 預處理01 指定時間區間票證資料切分\n",
    "#     pre01_split_ticket_with_day(selectdate_start, selectdate_end, selecttime_ticket_folder)\n",
    "\n",
    "#     # 預處理02: 過濾不合理票證資料(用站序資料)\n",
    "#     pre02_get_correct_tickets(selecttime_ticket_folder, checkok_ticketfolder)\n",
    "\n",
    "#     # 預處理03: 確認所有站點的經緯度在TDX都可以被核對出來\n",
    "#     pre03_findstops( checkok_ticketfolder, seqfolder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', 'TicketAnalysis', '00_TDX資料下載', '01公車站序資料')))\n",
    "\n",
    "#     # 預處理04: 加上必要欄位 (平假日欄位、刪除不重要的欄位）\n",
    "#     pre04_reformat(checkok_ticketfolder, reformat_folder, filterdate = ['2024-10-09', '2024-10-10', '2024-10-11', '2024-10-12', '2024-10-13', '2024-10-14', '2024-10-15'])\n",
    "\n",
    "#     # 分析01: 確認資料各票種、各路線、平假日、起點、迄點筆數 (避免後續處理原始票證資料，加速票證處理速度)\n",
    "#     analytics01_hourlycount(reformat_folder, \n",
    "#                             hourlycount_folder, \n",
    "#                             seqfolder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', 'TicketAnalysis', '00_TDX資料下載', '01公車站序資料')),\n",
    "#                             returndf=False)    \n",
    "\n",
    "#     # 預處理05: 重新比對站序\n",
    "    df, dftemp = pre05_redefined_stopsequence(outputfolder = hourlycount_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab65d8e2",
   "metadata": {},
   "source": [
    "# Unsured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f101367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_withhour(monthlist, \n",
    "                 seqfolder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', 'TicketAnalysis', '00_TDX資料下載', '01公車站序資料'))):\n",
    "    '''把站序跟Hour合在一起，並重新排序'''\n",
    "\n",
    "    df_seq = read_combined_dataframe(file_list=findfiles(seqfolder))\n",
    "    df_seq = df_seq.reindex(columns = ['RouteUID', 'RouteName_Zh', 'SubRouteUID', 'SubRouteName_Zh', 'Direction', 'StopUID', 'StopName_Zh', 'StopSequence', 'PositionLon', 'PositionLat'])\n",
    "    df_seq = df_seq.rename(columns = {'RouteName_Zh':'RouteName', \n",
    "                                    'SubRouteName_Zh':'SubRouteName',\n",
    "                                    'StopName_Zh':'StopName'})\n",
    "    \n",
    "    df_seq = df_seq.drop_duplicates(subset = ['RouteUID', 'SubRouteUID', 'Direction','StopSequence'])\n",
    "    df_hour = pd.DataFrame({'Hour': range(24)})\n",
    "    df_wdwk = pd.DataFrame({'WDWK': range(2)})\n",
    "    df_month = pd.DataFrame({'Month':monthlist})\n",
    "    df_month['Month'] = pd.to_datetime(df_month['Month']).dt.to_period('M')\n",
    "\n",
    "\n",
    "    df_hourseq = df_seq.merge(df_hour, how='cross')\n",
    "    df_hourseq = df_hourseq.merge(df_wdwk, how='cross')\n",
    "    df_hourseq = df_hourseq.merge(df_month, how='cross')\n",
    "\n",
    "\n",
    "    df_hourseq = df_hourseq.reindex(columns=['RouteUID', 'RouteName', 'SubRouteUID', 'SubRouteName', 'Month','WDWK', 'Hour', 'Direction', 'StopUID', 'StopName', 'StopSequence', 'PositionLon', 'PositionLat'])\n",
    "    df_hourseq = df_hourseq.sort_values(['RouteUID', 'SubRouteUID', 'SubRouteName', 'Month', 'WDWK', 'Hour', 'Direction', 'StopSequence']).reset_index(drop = True)\n",
    "\n",
    "    return df_hourseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df269dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(hourlycount_folder, '上下車區分票種分時計次.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfbackup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329776cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['DeboardingHour'] == -99]['Count'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533d09f",
   "metadata": {},
   "source": [
    "檢查中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90bca7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_on_and_off_count(hourlycount_folder):\n",
    "    '''分析02: 重新比對站序 & 處理站間量'''\n",
    "\n",
    "    df_hourseq = seq_withhour(monthlist=['2024-10', '2024-11'])\n",
    "\n",
    "    df = pd.read_csv(os.path.join(hourlycount_folder, '上下車區分票種分時計次.csv'))\n",
    "\n",
    "    df['InfoDate'] = pd.to_datetime(df['InfoDate'], errors='coerce')\n",
    "    df['Month'] = df['InfoDate'].dt.to_period('M')\n",
    "    reindexcolumns = ['InfoDate', 'Month', 'WDWK', 'BoardingHour', 'DeboardingHour',\n",
    "                      'RouteUID', 'RouteName', 'SubRouteUID', 'SubRouteName', 'Direction',\n",
    "                      'BoardingStopUID', 'BoardingStopName', 'BoardingStopSequence',\n",
    "                      'DeboardingStopUID', 'DeboardingStopName', 'DeboardingStopSequence',\n",
    "                      'Count']\n",
    "    df = df.reindex(columns=reindexcolumns)\n",
    "\n",
    "    # 針對原始的OD起訖資料進行過濾\n",
    "    df = df[df['WDWK'].isin([0, 1])]\n",
    "    df = df[(df['BoardingHour'] != -99) & (df['DeboardingHour'] != -99)]\n",
    "\n",
    "    # === 只保留「上/下車鍵都能配對 df_hourseq」的列（最小改動）===\n",
    "    hourseq_keys = df_hourseq[['Month','WDWK','Hour','RouteUID','SubRouteUID','Direction','StopSequence']].drop_duplicates()\n",
    "    df = df.reset_index(drop=False).rename(columns={'index': 'row_id'})\n",
    "\n",
    "    board_ok = (\n",
    "        df[['row_id','Month','WDWK','BoardingHour','RouteUID','SubRouteUID','Direction','BoardingStopSequence']]\n",
    "        .merge(hourseq_keys,\n",
    "               left_on = ['Month','WDWK','BoardingHour','RouteUID','SubRouteUID','Direction','BoardingStopSequence'],\n",
    "               right_on = ['Month','WDWK','Hour','RouteUID','SubRouteUID','Direction','StopSequence'],\n",
    "               how='inner')[['row_id']].drop_duplicates()\n",
    "    )\n",
    "    deboard_ok = (\n",
    "        df[['row_id','Month','WDWK','DeboardingHour','RouteUID','SubRouteUID','Direction','DeboardingStopSequence']]\n",
    "        .merge(hourseq_keys,\n",
    "               left_on = ['Month','WDWK','DeboardingHour','RouteUID','SubRouteUID','Direction','DeboardingStopSequence'],\n",
    "               right_on = ['Month','WDWK','Hour','RouteUID','SubRouteUID','Direction','StopSequence'],\n",
    "               how='inner')[['row_id']].drop_duplicates()\n",
    "    )\n",
    "    valid_ids = pd.Index(board_ok['row_id']).intersection(deboard_ok['row_id'])\n",
    "    df = df[df['row_id'].isin(valid_ids)].drop(columns='row_id')\n",
    "    \n",
    "    # 將上下車資料黏貼到站序\n",
    "    df_on = df.reindex(columns=['Month','WDWK','BoardingHour','Direction','RouteUID','SubRouteUID','BoardingStopSequence','Count']) \\\n",
    "              .rename(columns={'BoardingHour':'Hour','BoardingStopSequence':'StopSequence','Count':'On'})\n",
    "    on_groupby_columns = [c for c in df_on.columns if c != \"On\"]\n",
    "    df_on[on_groupby_columns] = df_on[on_groupby_columns].fillna('-99')\n",
    "    df_on = df_on.groupby(on_groupby_columns).agg({'On':'sum'}).reset_index()\n",
    "\n",
    "    df_off = df.reindex(columns=['Month','WDWK','DeboardingHour','Direction','RouteUID','SubRouteUID','DeboardingStopSequence','Count']) \\\n",
    "               .rename(columns={'DeboardingHour':'Hour','DeboardingStopSequence':'StopSequence','Count':'Off'})\n",
    "    off_groupby_columns = [c for c in df_off.columns if c != \"Off\"]\n",
    "    df_off[off_groupby_columns] = df_off[off_groupby_columns].fillna('-99')\n",
    "    df_off = df_off.groupby(off_groupby_columns).agg({'Off':'sum'}).reset_index()\n",
    "\n",
    "    dfcount = pd.merge(df_hourseq, df_on,\n",
    "                       on=['Month','WDWK','Hour','RouteUID','SubRouteUID','Direction','StopSequence'],\n",
    "                       how='left')\n",
    "    dfcount = pd.merge(dfcount, df_off,\n",
    "                       on=['Month','WDWK','Hour','RouteUID','SubRouteUID','Direction','StopSequence'],\n",
    "                       how='left')\n",
    "    dfcount[['On','Off']] = dfcount[['On','Off']].fillna(0)\n",
    "\n",
    "    return dfcount, df_on, df_off\n",
    "\n",
    "def calc_onbus_pd(df, oncolumn, offcolumn, seqcolumn, outcolumn=\"OnBus\"):\n",
    "    # 以 NumPy 陣列計算，避免中間欄位\n",
    "    on  = df[oncolumn].to_numpy()\n",
    "    off = df[offcolumn].to_numpy()\n",
    "    seq = df[seqcolumn].to_numpy()\n",
    "\n",
    "    delta = on - off                                  # n-element array\n",
    "    seg_id = (seq == 1).cumsum()                      # 1,1,1,2,2,2,...\n",
    "\n",
    "    # pandas 的 groupby(...).cumsum() 是 Cython 寫的，速度遠勝 apply\n",
    "    out = pd.Series(delta).groupby(seg_id).cumsum().to_numpy()\n",
    "\n",
    "    # clip 負值\n",
    "    np.maximum(out, 0, out)                           # in-place\n",
    "\n",
    "    df[outcolumn] = out\n",
    "    return df\n",
    "\n",
    "def analytics02_onbus_count(hourlycount_folder, dailybetweenstops_folder):\n",
    "    # 調整為上下車站序統計表\n",
    "    dfcount, df_on, df_off = seq_on_and_off_count(hourlycount_folder)\n",
    "\n",
    "    # 計算站間量\n",
    "    betweenstops = calc_onbus_pd(df = dfcount, \n",
    "                                 oncolumn = 'On', offcolumn = 'Off', \n",
    "                                 seqcolumn='StopSequence', \n",
    "                                 outcolumn='OnBus')\n",
    "    \n",
    "    # 取每日平均\n",
    "    df_date = pd.read_excel(os.path.join(referencefolder, 'Date.xlsx'), sheet_name = 'DateCount')\n",
    "    df_date['Month'] = pd.to_datetime(df_date['Month']).dt.to_period('M')\n",
    "    betweenstops = pd.merge(betweenstops, df_date, on = ['Month', 'WDWK'])\n",
    "    betweenstops[['On_AVG', 'Off_AVG']] = (\n",
    "        betweenstops[['On', 'Off']].div(betweenstops['DayCount'], axis=0).round(2)\n",
    "    )\n",
    "\n",
    "    # 輸出\n",
    "    betweenstops_outputfile = os.path.join(dailybetweenstops_folder, '全日站間量.csv')\n",
    "    betweenstops.to_csv(betweenstops_outputfile, index=False, encoding='utf-8-sig')\n",
    "    return betweenstops \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5238a7b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 194. MiB for an array with shape (6, 4239456) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_hourseq = \u001b[43mseq_withhour\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonthlist\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2024-10\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2024-11\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mseq_withhour\u001b[39m\u001b[34m(monthlist, seqfolder)\u001b[39m\n\u001b[32m     14\u001b[39m df_month = pd.DataFrame({\u001b[33m'\u001b[39m\u001b[33mMonth\u001b[39m\u001b[33m'\u001b[39m:monthlist})\n\u001b[32m     15\u001b[39m df_month[\u001b[33m'\u001b[39m\u001b[33mMonth\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df_month[\u001b[33m'\u001b[39m\u001b[33mMonth\u001b[39m\u001b[33m'\u001b[39m]).dt.to_period(\u001b[33m'\u001b[39m\u001b[33mM\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m df_hourseq = \u001b[43mdf_seq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_hour\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcross\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m df_hourseq = df_hourseq.merge(df_wdwk, how=\u001b[33m'\u001b[39m\u001b[33mcross\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     20\u001b[39m df_hourseq = df_hourseq.merge(df_month, how=\u001b[33m'\u001b[39m\u001b[33mcross\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:10839\u001b[39m, in \u001b[36mDataFrame.merge\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10820\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m  10821\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents=\u001b[32m2\u001b[39m)\n\u001b[32m  10822\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m  10835\u001b[39m     validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10836\u001b[39m ) -> DataFrame:\n\u001b[32m  10837\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m> \u001b[39m\u001b[32m10839\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10840\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10848\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10849\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10853\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:155\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    153\u001b[39m right_df = _validate_operand(right)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m how == \u001b[33m\"\u001b[39m\u001b[33mcross\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cross_merge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     op = _MergeOperation(\n\u001b[32m    171\u001b[39m         left_df,\n\u001b[32m    172\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m         validate=validate,\n\u001b[32m    183\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:223\u001b[39m, in \u001b[36m_cross_merge\u001b[39m\u001b[34m(left, right, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    219\u001b[39m right = right.assign(**{cross_col: \u001b[32m1\u001b[39m})\n\u001b[32m    221\u001b[39m left_on = right_on = [cross_col]\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m res = \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minner\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m res[cross_col]\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     op = _MergeOperation(\n\u001b[32m    171\u001b[39m         left_df,\n\u001b[32m    172\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m         validate=validate,\n\u001b[32m    183\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:888\u001b[39m, in \u001b[36m_MergeOperation.get_result\u001b[39m\u001b[34m(self, copy)\u001b[39m\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right = \u001b[38;5;28mself\u001b[39m._indicator_pre_merge(\u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right)\n\u001b[32m    886\u001b[39m join_index, left_indexer, right_indexer = \u001b[38;5;28mself\u001b[39m._get_join_info()\n\u001b[32m--> \u001b[39m\u001b[32m888\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[38;5;28mself\u001b[39m._merge_type)\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.indicator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:848\u001b[39m, in \u001b[36m_MergeOperation._reindex_and_concat\u001b[39m\u001b[34m(self, join_index, left_indexer, right_indexer, copy)\u001b[39m\n\u001b[32m    840\u001b[39m llabels, rlabels = _items_overlap_with_suffix(\n\u001b[32m    841\u001b[39m     \u001b[38;5;28mself\u001b[39m.left._info_axis, \u001b[38;5;28mself\u001b[39m.right._info_axis, \u001b[38;5;28mself\u001b[39m.suffixes\n\u001b[32m    842\u001b[39m )\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[32m    845\u001b[39m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[32m    846\u001b[39m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[32m    847\u001b[39m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m     lmgr = \u001b[43mleft\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    857\u001b[39m     left = left._constructor_from_mgr(lmgr, axes=lmgr.axes)\n\u001b[32m    858\u001b[39m left.index = join_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:687\u001b[39m, in \u001b[36mBaseBlockManager.reindex_indexer\u001b[39m\u001b[34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[39m\n\u001b[32m    680\u001b[39m     new_blocks = \u001b[38;5;28mself\u001b[39m._slice_take_blocks_ax0(\n\u001b[32m    681\u001b[39m         indexer,\n\u001b[32m    682\u001b[39m         fill_value=fill_value,\n\u001b[32m    683\u001b[39m         only_slice=only_slice,\n\u001b[32m    684\u001b[39m         use_na_proxy=use_na_proxy,\n\u001b[32m    685\u001b[39m     )\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m     new_blocks = \u001b[43m[\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    698\u001b[39m new_axes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m    699\u001b[39m new_axes[axis] = new_axis\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:688\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    680\u001b[39m     new_blocks = \u001b[38;5;28mself\u001b[39m._slice_take_blocks_ax0(\n\u001b[32m    681\u001b[39m         indexer,\n\u001b[32m    682\u001b[39m         fill_value=fill_value,\n\u001b[32m    683\u001b[39m         only_slice=only_slice,\n\u001b[32m    684\u001b[39m         use_na_proxy=use_na_proxy,\n\u001b[32m    685\u001b[39m     )\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    687\u001b[39m     new_blocks = [\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m         \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    695\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks\n\u001b[32m    696\u001b[39m     ]\n\u001b[32m    698\u001b[39m new_axes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m    699\u001b[39m new_axes[axis] = new_axis\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\blocks.py:1373\u001b[39m, in \u001b[36mBlock.take_nd\u001b[39m\u001b[34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[39m\n\u001b[32m   1370\u001b[39m     allow_fill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1372\u001b[39m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1373\u001b[39m new_values = \u001b[43malgos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[32m   1378\u001b[39m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[32m   1380\u001b[39m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[32m   1381\u001b[39m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[39m, in \u001b[36mtake_nd\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001b[32m    116\u001b[39m arr = np.asarray(arr)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    155\u001b[39m     out = np.empty(out_shape, dtype=dtype, order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     out = np.empty(out_shape, dtype=dtype)\n\u001b[32m    159\u001b[39m func = _get_take_nd_function(\n\u001b[32m    160\u001b[39m     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n\u001b[32m    161\u001b[39m )\n\u001b[32m    162\u001b[39m func(arr, indexer, out, fill_value)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 194. MiB for an array with shape (6, 4239456) and data type object"
     ]
    }
   ],
   "source": [
    "df_hourseq = seq_withhour(monthlist=['2024-10', '2024-11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97034edc",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 647. MiB for an array with shape (5, 16957824) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43manalytics02_onbus_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhourlycount_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdailybetweenstops_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36manalytics02_onbus_count\u001b[39m\u001b[34m(hourlycount_folder, dailybetweenstops_folder)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manalytics02_onbus_count\u001b[39m(hourlycount_folder, dailybetweenstops_folder):\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# 調整為上下車站序統計表\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     dfcount, df_on, df_off = \u001b[43mseq_on_and_off_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhourlycount_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# 計算站間量\u001b[39;00m\n\u001b[32m     88\u001b[39m     betweenstops = calc_onbus_pd(df = dfcount, \n\u001b[32m     89\u001b[39m                                  oncolumn = \u001b[33m'\u001b[39m\u001b[33mOn\u001b[39m\u001b[33m'\u001b[39m, offcolumn = \u001b[33m'\u001b[39m\u001b[33mOff\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     90\u001b[39m                                  seqcolumn=\u001b[33m'\u001b[39m\u001b[33mStopSequence\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     91\u001b[39m                                  outcolumn=\u001b[33m'\u001b[39m\u001b[33mOnBus\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mseq_on_and_off_count\u001b[39m\u001b[34m(hourlycount_folder)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mseq_on_and_off_count\u001b[39m(hourlycount_folder):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''分析02: 重新比對站序 & 處理站間量'''\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     df_hourseq = \u001b[43mseq_withhour\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonthlist\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2024-10\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2024-11\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     df = pd.read_csv(os.path.join(hourlycount_folder, \u001b[33m'\u001b[39m\u001b[33m上下車區分票種分時計次.csv\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      8\u001b[39m     df[\u001b[33m'\u001b[39m\u001b[33mInfoDate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33mInfoDate\u001b[39m\u001b[33m'\u001b[39m], errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mseq_withhour\u001b[39m\u001b[34m(monthlist, seqfolder)\u001b[39m\n\u001b[32m     18\u001b[39m df_hourseq = df_seq.merge(df_hour, how=\u001b[33m'\u001b[39m\u001b[33mcross\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     19\u001b[39m df_hourseq = df_hourseq.merge(df_wdwk, how=\u001b[33m'\u001b[39m\u001b[33mcross\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m df_hourseq = \u001b[43mdf_hourseq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_month\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcross\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m df_hourseq = df_hourseq.reindex(columns=[\u001b[33m'\u001b[39m\u001b[33mRouteUID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mRouteName\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSubRouteUID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSubRouteName\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMonth\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mWDWK\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mHour\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDirection\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mStopUID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mStopName\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mStopSequence\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPositionLon\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPositionLat\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     24\u001b[39m df_hourseq = df_hourseq.sort_values([\u001b[33m'\u001b[39m\u001b[33mRouteUID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSubRouteUID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSubRouteName\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMonth\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mWDWK\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mHour\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDirection\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mStopSequence\u001b[39m\u001b[33m'\u001b[39m]).reset_index(drop = \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:10839\u001b[39m, in \u001b[36mDataFrame.merge\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10820\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m  10821\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents=\u001b[32m2\u001b[39m)\n\u001b[32m  10822\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m  10835\u001b[39m     validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10836\u001b[39m ) -> DataFrame:\n\u001b[32m  10837\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m> \u001b[39m\u001b[32m10839\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10840\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10848\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10849\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10853\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:155\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    153\u001b[39m right_df = _validate_operand(right)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m how == \u001b[33m\"\u001b[39m\u001b[33mcross\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cross_merge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     op = _MergeOperation(\n\u001b[32m    171\u001b[39m         left_df,\n\u001b[32m    172\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m         validate=validate,\n\u001b[32m    183\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:223\u001b[39m, in \u001b[36m_cross_merge\u001b[39m\u001b[34m(left, right, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    219\u001b[39m right = right.assign(**{cross_col: \u001b[32m1\u001b[39m})\n\u001b[32m    221\u001b[39m left_on = right_on = [cross_col]\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m res = \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minner\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m res[cross_col]\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     op = _MergeOperation(\n\u001b[32m    171\u001b[39m         left_df,\n\u001b[32m    172\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m         validate=validate,\n\u001b[32m    183\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:888\u001b[39m, in \u001b[36m_MergeOperation.get_result\u001b[39m\u001b[34m(self, copy)\u001b[39m\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right = \u001b[38;5;28mself\u001b[39m._indicator_pre_merge(\u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right)\n\u001b[32m    886\u001b[39m join_index, left_indexer, right_indexer = \u001b[38;5;28mself\u001b[39m._get_join_info()\n\u001b[32m--> \u001b[39m\u001b[32m888\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[38;5;28mself\u001b[39m._merge_type)\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.indicator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:879\u001b[39m, in \u001b[36m_MergeOperation._reindex_and_concat\u001b[39m\u001b[34m(self, join_index, left_indexer, right_indexer, copy)\u001b[39m\n\u001b[32m    877\u001b[39m left.columns = llabels\n\u001b[32m    878\u001b[39m right.columns = rlabels\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m result = \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    680\u001b[39m             indexers[ax] = obj_labels.get_indexer(new_labels)\n\u001b[32m    682\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m new_data = \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[32m    688\u001b[39m     new_data._consolidate_inplace()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\concat.py:131\u001b[39m, in \u001b[36mconcatenate_managers\u001b[39m\u001b[34m(mgrs_indexers, axes, concat_axis, copy)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m concat_axis == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     mgrs = \u001b[43m_maybe_reindex_columns_na_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[32m0\u001b[39m].concat_horizontal(mgrs, axes)\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].nblocks > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\concat.py:230\u001b[39m, in \u001b[36m_maybe_reindex_columns_na_proxy\u001b[39m\u001b[34m(axes, mgrs_indexers, needs_copy)\u001b[39m\n\u001b[32m    220\u001b[39m         mgr = mgr.reindex_indexer(\n\u001b[32m    221\u001b[39m             axes[i],\n\u001b[32m    222\u001b[39m             indexers[i],\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m             use_na_proxy=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[32m    228\u001b[39m         )\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m         mgr = \u001b[43mmgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m     new_mgrs.append(mgr)\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:604\u001b[39m, in \u001b[36mBaseBlockManager.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    601\u001b[39m         res._blklocs = \u001b[38;5;28mself\u001b[39m._blklocs.copy()\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[39m, in \u001b[36mBlockManager._consolidate_inplace\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1783\u001b[39m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[32m   1784\u001b[39m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[32m   1785\u001b[39m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[32m   1786\u001b[39m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[32m   1787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_consolidated():\n\u001b[32m-> \u001b[39m\u001b[32m1788\u001b[39m         \u001b[38;5;28mself\u001b[39m.blocks = \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m         \u001b[38;5;28mself\u001b[39m._is_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1790\u001b[39m         \u001b[38;5;28mself\u001b[39m._known_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[39m, in \u001b[36m_consolidate\u001b[39m\u001b[34m(blocks)\u001b[39m\n\u001b[32m   2267\u001b[39m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   2268\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[32m-> \u001b[39m\u001b[32m2269\u001b[39m     merged_blocks, _ = \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2270\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[32m   2271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2272\u001b[39m     new_blocks = extend_blocks(merged_blocks, new_blocks)\n\u001b[32m   2273\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:2301\u001b[39m, in \u001b[36m_merge_blocks\u001b[39m\u001b[34m(blocks, dtype, can_consolidate)\u001b[39m\n\u001b[32m   2298\u001b[39m     new_values = bvals2[\u001b[32m0\u001b[39m]._concat_same_type(bvals2, axis=\u001b[32m0\u001b[39m)\n\u001b[32m   2300\u001b[39m argsort = np.argsort(new_mgr_locs)\n\u001b[32m-> \u001b[39m\u001b[32m2301\u001b[39m new_values = \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2302\u001b[39m new_mgr_locs = new_mgr_locs[argsort]\n\u001b[32m   2304\u001b[39m bp = BlockPlacement(new_mgr_locs)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 647. MiB for an array with shape (5, 16957824) and data type int64"
     ]
    }
   ],
   "source": [
    "analytics02_onbus_count(hourlycount_folder, dailybetweenstops_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6889557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenstops = calc_onbus_pd(df = dfcount, \n",
    "                       oncolumn = 'On', offcolumn = 'Off', \n",
    "                       seqcolumn='StopSequence', \n",
    "                       outcolumn='OnBus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9148484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b0a81baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RouteUID</th>\n",
       "      <th>RouteName</th>\n",
       "      <th>SubRouteUID</th>\n",
       "      <th>SubRouteName</th>\n",
       "      <th>Month</th>\n",
       "      <th>WDWK</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Direction</th>\n",
       "      <th>StopUID</th>\n",
       "      <th>StopName</th>\n",
       "      <th>StopSequence</th>\n",
       "      <th>PositionLon</th>\n",
       "      <th>PositionLat</th>\n",
       "      <th>On</th>\n",
       "      <th>Off</th>\n",
       "      <th>OnBus</th>\n",
       "      <th>DayCount</th>\n",
       "      <th>On_AVG</th>\n",
       "      <th>Off_AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KEE0155</td>\n",
       "      <td>3044</td>\n",
       "      <td>KEE015501</td>\n",
       "      <td>3044</td>\n",
       "      <td>2024-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>KEE306661</td>\n",
       "      <td>基隆火車站(南站)</td>\n",
       "      <td>1</td>\n",
       "      <td>121.738670</td>\n",
       "      <td>25.131280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KEE0155</td>\n",
       "      <td>3044</td>\n",
       "      <td>KEE015501</td>\n",
       "      <td>3044</td>\n",
       "      <td>2024-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>KEE306737</td>\n",
       "      <td>孝二路</td>\n",
       "      <td>2</td>\n",
       "      <td>121.740275</td>\n",
       "      <td>25.129851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KEE0155</td>\n",
       "      <td>3044</td>\n",
       "      <td>KEE015501</td>\n",
       "      <td>3044</td>\n",
       "      <td>2024-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>KEE306736</td>\n",
       "      <td>三十一號橋</td>\n",
       "      <td>3</td>\n",
       "      <td>121.736985</td>\n",
       "      <td>25.130240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KEE0155</td>\n",
       "      <td>3044</td>\n",
       "      <td>KEE015501</td>\n",
       "      <td>3044</td>\n",
       "      <td>2024-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>KEE309534</td>\n",
       "      <td>基隆轉運站</td>\n",
       "      <td>4</td>\n",
       "      <td>121.739089</td>\n",
       "      <td>25.133076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KEE0155</td>\n",
       "      <td>3044</td>\n",
       "      <td>KEE015501</td>\n",
       "      <td>3044</td>\n",
       "      <td>2024-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>KEE309535</td>\n",
       "      <td>民治里活動中心</td>\n",
       "      <td>5</td>\n",
       "      <td>121.740028</td>\n",
       "      <td>25.134482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16957819</th>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>2024-11</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>TPE18255</td>\n",
       "      <td>捷運六張犁站(和平)</td>\n",
       "      <td>41</td>\n",
       "      <td>121.553535</td>\n",
       "      <td>25.023243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16957820</th>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>2024-11</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>TPE18257</td>\n",
       "      <td>富陽街口</td>\n",
       "      <td>42</td>\n",
       "      <td>121.555911</td>\n",
       "      <td>25.021034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16957821</th>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>2024-11</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>TPE18259</td>\n",
       "      <td>黎忠市場</td>\n",
       "      <td>43</td>\n",
       "      <td>121.557804</td>\n",
       "      <td>25.019261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16957822</th>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>2024-11</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>TPE57540</td>\n",
       "      <td>捷運麟光站</td>\n",
       "      <td>44</td>\n",
       "      <td>121.558530</td>\n",
       "      <td>25.018250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16957823</th>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>2024-11</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>TPE18261</td>\n",
       "      <td>麟光站</td>\n",
       "      <td>45</td>\n",
       "      <td>121.561041</td>\n",
       "      <td>25.015473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16957824 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         RouteUID RouteName SubRouteUID SubRouteName    Month  WDWK  Hour  \\\n",
       "0         KEE0155      3044   KEE015501         3044  2024-10     0     0   \n",
       "1         KEE0155      3044   KEE015501         3044  2024-10     0     0   \n",
       "2         KEE0155      3044   KEE015501         3044  2024-10     0     0   \n",
       "3         KEE0155      3044   KEE015501         3044  2024-10     0     0   \n",
       "4         KEE0155      3044   KEE015501         3044  2024-10     0     0   \n",
       "...           ...       ...         ...          ...      ...   ...   ...   \n",
       "16957819   TPE810      敦化幹線      TPE810         敦化幹線  2024-11     1    23   \n",
       "16957820   TPE810      敦化幹線      TPE810         敦化幹線  2024-11     1    23   \n",
       "16957821   TPE810      敦化幹線      TPE810         敦化幹線  2024-11     1    23   \n",
       "16957822   TPE810      敦化幹線      TPE810         敦化幹線  2024-11     1    23   \n",
       "16957823   TPE810      敦化幹線      TPE810         敦化幹線  2024-11     1    23   \n",
       "\n",
       "          Direction    StopUID    StopName  StopSequence  PositionLon  \\\n",
       "0                 0  KEE306661   基隆火車站(南站)             1   121.738670   \n",
       "1                 0  KEE306737         孝二路             2   121.740275   \n",
       "2                 0  KEE306736       三十一號橋             3   121.736985   \n",
       "3                 0  KEE309534       基隆轉運站             4   121.739089   \n",
       "4                 0  KEE309535     民治里活動中心             5   121.740028   \n",
       "...             ...        ...         ...           ...          ...   \n",
       "16957819          1   TPE18255  捷運六張犁站(和平)            41   121.553535   \n",
       "16957820          1   TPE18257        富陽街口            42   121.555911   \n",
       "16957821          1   TPE18259        黎忠市場            43   121.557804   \n",
       "16957822          1   TPE57540       捷運麟光站            44   121.558530   \n",
       "16957823          1   TPE18261         麟光站            45   121.561041   \n",
       "\n",
       "          PositionLat   On  Off  OnBus  DayCount  On_AVG  Off_AVG  \n",
       "0           25.131280  0.0  0.0    0.0         6     0.0      0.0  \n",
       "1           25.129851  0.0  0.0    0.0         6     0.0      0.0  \n",
       "2           25.130240  0.0  0.0    0.0         6     0.0      0.0  \n",
       "3           25.133076  0.0  0.0    0.0         6     0.0      0.0  \n",
       "4           25.134482  0.0  0.0    0.0         6     0.0      0.0  \n",
       "...               ...  ...  ...    ...       ...     ...      ...  \n",
       "16957819    25.023243  0.0  0.0    0.0        12     0.0      0.0  \n",
       "16957820    25.021034  0.0  0.0    0.0        12     0.0      0.0  \n",
       "16957821    25.019261  0.0  0.0    0.0        12     0.0      0.0  \n",
       "16957822    25.018250  0.0  0.0    0.0        12     0.0      0.0  \n",
       "16957823    25.015473  0.0  0.0    0.0        12     0.0      0.0  \n",
       "\n",
       "[16957824 rows x 19 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betweenstops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af50f4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16957824 entries, 0 to 16957823\n",
      "Data columns (total 16 columns):\n",
      " #   Column        Dtype    \n",
      "---  ------        -----    \n",
      " 0   RouteUID      object   \n",
      " 1   RouteName     object   \n",
      " 2   SubRouteUID   object   \n",
      " 3   SubRouteName  object   \n",
      " 4   Month         period[M]\n",
      " 5   WDWK          int64    \n",
      " 6   Hour          int64    \n",
      " 7   Direction     int64    \n",
      " 8   StopUID       object   \n",
      " 9   StopName      object   \n",
      " 10  StopSequence  int64    \n",
      " 11  PositionLon   float64  \n",
      " 12  PositionLat   float64  \n",
      " 13  On            float64  \n",
      " 14  Off           float64  \n",
      " 15  OnBus         float64  \n",
      "dtypes: float64(5), int64(4), object(6), period[M](1)\n",
      "memory usage: 2.0+ GB\n"
     ]
    }
   ],
   "source": [
    "betweenstops.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f86c15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RouteUID</th>\n",
       "      <th>RouteName</th>\n",
       "      <th>SubRouteUID</th>\n",
       "      <th>SubRouteName</th>\n",
       "      <th>Month</th>\n",
       "      <th>WDWK</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Direction</th>\n",
       "      <th>StopUID</th>\n",
       "      <th>StopName</th>\n",
       "      <th>StopSequence</th>\n",
       "      <th>PositionLon</th>\n",
       "      <th>PositionLat</th>\n",
       "      <th>On</th>\n",
       "      <th>Off</th>\n",
       "      <th>OnBus</th>\n",
       "      <th>DayCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KEE0155</td>\n",
       "      <td>3044</td>\n",
       "      <td>KEE015501</td>\n",
       "      <td>3044</td>\n",
       "      <td>2024-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>KEE306661</td>\n",
       "      <td>基隆火車站(南站)</td>\n",
       "      <td>1</td>\n",
       "      <td>121.738670</td>\n",
       "      <td>25.131280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KEE0155</td>\n",
       "      <td>3044</td>\n",
       "      <td>KEE015501</td>\n",
       "      <td>3044</td>\n",
       "      <td>2024-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>KEE306737</td>\n",
       "      <td>孝二路</td>\n",
       "      <td>2</td>\n",
       "      <td>121.740275</td>\n",
       "      <td>25.129851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KEE0155</td>\n",
       "      <td>3044</td>\n",
       "      <td>KEE015501</td>\n",
       "      <td>3044</td>\n",
       "      <td>2024-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>KEE306736</td>\n",
       "      <td>三十一號橋</td>\n",
       "      <td>3</td>\n",
       "      <td>121.736985</td>\n",
       "      <td>25.130240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KEE0155</td>\n",
       "      <td>3044</td>\n",
       "      <td>KEE015501</td>\n",
       "      <td>3044</td>\n",
       "      <td>2024-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>KEE309534</td>\n",
       "      <td>基隆轉運站</td>\n",
       "      <td>4</td>\n",
       "      <td>121.739089</td>\n",
       "      <td>25.133076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KEE0155</td>\n",
       "      <td>3044</td>\n",
       "      <td>KEE015501</td>\n",
       "      <td>3044</td>\n",
       "      <td>2024-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>KEE309535</td>\n",
       "      <td>民治里活動中心</td>\n",
       "      <td>5</td>\n",
       "      <td>121.740028</td>\n",
       "      <td>25.134482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16957819</th>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>2024-11</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>TPE18255</td>\n",
       "      <td>捷運六張犁站(和平)</td>\n",
       "      <td>41</td>\n",
       "      <td>121.553535</td>\n",
       "      <td>25.023243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16957820</th>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>2024-11</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>TPE18257</td>\n",
       "      <td>富陽街口</td>\n",
       "      <td>42</td>\n",
       "      <td>121.555911</td>\n",
       "      <td>25.021034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16957821</th>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>2024-11</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>TPE18259</td>\n",
       "      <td>黎忠市場</td>\n",
       "      <td>43</td>\n",
       "      <td>121.557804</td>\n",
       "      <td>25.019261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16957822</th>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>2024-11</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>TPE57540</td>\n",
       "      <td>捷運麟光站</td>\n",
       "      <td>44</td>\n",
       "      <td>121.558530</td>\n",
       "      <td>25.018250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16957823</th>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>TPE810</td>\n",
       "      <td>敦化幹線</td>\n",
       "      <td>2024-11</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>TPE18261</td>\n",
       "      <td>麟光站</td>\n",
       "      <td>45</td>\n",
       "      <td>121.561041</td>\n",
       "      <td>25.015473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16957824 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         RouteUID RouteName SubRouteUID SubRouteName    Month  WDWK  Hour  \\\n",
       "0         KEE0155      3044   KEE015501         3044  2024-10     0     0   \n",
       "1         KEE0155      3044   KEE015501         3044  2024-10     0     0   \n",
       "2         KEE0155      3044   KEE015501         3044  2024-10     0     0   \n",
       "3         KEE0155      3044   KEE015501         3044  2024-10     0     0   \n",
       "4         KEE0155      3044   KEE015501         3044  2024-10     0     0   \n",
       "...           ...       ...         ...          ...      ...   ...   ...   \n",
       "16957819   TPE810      敦化幹線      TPE810         敦化幹線  2024-11     1    23   \n",
       "16957820   TPE810      敦化幹線      TPE810         敦化幹線  2024-11     1    23   \n",
       "16957821   TPE810      敦化幹線      TPE810         敦化幹線  2024-11     1    23   \n",
       "16957822   TPE810      敦化幹線      TPE810         敦化幹線  2024-11     1    23   \n",
       "16957823   TPE810      敦化幹線      TPE810         敦化幹線  2024-11     1    23   \n",
       "\n",
       "          Direction    StopUID    StopName  StopSequence  PositionLon  \\\n",
       "0                 0  KEE306661   基隆火車站(南站)             1   121.738670   \n",
       "1                 0  KEE306737         孝二路             2   121.740275   \n",
       "2                 0  KEE306736       三十一號橋             3   121.736985   \n",
       "3                 0  KEE309534       基隆轉運站             4   121.739089   \n",
       "4                 0  KEE309535     民治里活動中心             5   121.740028   \n",
       "...             ...        ...         ...           ...          ...   \n",
       "16957819          1   TPE18255  捷運六張犁站(和平)            41   121.553535   \n",
       "16957820          1   TPE18257        富陽街口            42   121.555911   \n",
       "16957821          1   TPE18259        黎忠市場            43   121.557804   \n",
       "16957822          1   TPE57540       捷運麟光站            44   121.558530   \n",
       "16957823          1   TPE18261         麟光站            45   121.561041   \n",
       "\n",
       "          PositionLat   On  Off  OnBus  DayCount  \n",
       "0           25.131280  0.0  0.0    0.0         6  \n",
       "1           25.129851  0.0  0.0    0.0         6  \n",
       "2           25.130240  0.0  0.0    0.0         6  \n",
       "3           25.133076  0.0  0.0    0.0         6  \n",
       "4           25.134482  0.0  0.0    0.0         6  \n",
       "...               ...  ...  ...    ...       ...  \n",
       "16957819    25.023243  0.0  0.0    0.0        12  \n",
       "16957820    25.021034  0.0  0.0    0.0        12  \n",
       "16957821    25.019261  0.0  0.0    0.0        12  \n",
       "16957822    25.018250  0.0  0.0    0.0        12  \n",
       "16957823    25.015473  0.0  0.0    0.0        12  \n",
       "\n",
       "[16957824 rows x 17 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ce62e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>WDWK</th>\n",
       "      <th>DayCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-11</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Month  WDWK  DayCount\n",
       "0  2024-10     1        12\n",
       "1  2024-10    -1         6\n",
       "2  2024-10     0         6\n",
       "3  2024-11    -1         9\n",
       "4  2024-11     0         9\n",
       "5  2024-11     1        12"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "844bbc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcountbackup = dfcount.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算站間累計人數\n",
    "for i in range(len(dfcount)):\n",
    "    if dfcount.loc[i, 'StopSequence'] == 1:\n",
    "        dfcount.loc[i, 'OnBus'] = dfcount.loc[i, 'On'] - dfcount.loc[i, 'Off']\n",
    "    else:\n",
    "        dfcount.loc[i, 'OnBus'] = dfcount.loc[i - 1, 'On'] + dfcount.loc[i, 'On'] - dfcount.loc[i, 'Off']\n",
    "dfcount.loc[dfcount['OnBus'] < 0, 'OnBus'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02510a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcount"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
