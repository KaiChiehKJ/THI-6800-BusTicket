{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51662c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "from collections import Counter   # 用來方便累加每個 chunk 的統計結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873ae102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00_setup_os處理函數\n",
    "def create_folder(folder_name):\n",
    "    \"\"\"建立資料夾\"\"\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    return os.path.abspath(folder_name)\n",
    "\n",
    "def findfiles(filefolderpath, filetype='.csv', recursive=True):\n",
    "    \"\"\"\n",
    "    尋找指定路徑下指定類型的檔案，並返回檔案路徑列表。\n",
    "\n",
    "    Args:\n",
    "        filefolderpath (str): 指定的檔案路徑。\n",
    "        filetype (str, optional): 要尋找的檔案類型，預設為 '.csv'。\n",
    "        recursive (bool, optional): 是否檢索所有子資料夾，預設為 True；反之為False，僅查找當前資料夾的所有file。\n",
    "\n",
    "    Returns:\n",
    "        list: 包含所有符合條件的檔案路徑的列表。\n",
    "    \"\"\"\n",
    "    filelist = []\n",
    "\n",
    "    if recursive:\n",
    "        # 遍歷資料夾及其子資料夾\n",
    "        for root, _, files in os.walk(filefolderpath):\n",
    "            for file in files:\n",
    "                if file.endswith(filetype):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    filelist.append(file_path)\n",
    "    else:\n",
    "        # 僅檢索當前資料夾\n",
    "        for file in os.listdir(filefolderpath):\n",
    "            file_path = os.path.join(filefolderpath, file)\n",
    "            if os.path.isfile(file_path) and file.endswith(filetype):\n",
    "                filelist.append(file_path)\n",
    "\n",
    "    return filelist\n",
    "\n",
    "def read_combined_dataframe(file_list, filepath = True):\n",
    "    dataframes = []\n",
    "    \n",
    "    for file in file_list:\n",
    "        try:\n",
    "            if file.endswith('.csv'):\n",
    "                df = pd.read_csv(file)\n",
    "            elif file.endswith('.shp'):\n",
    "                df = gpd.read_file(file)\n",
    "            elif file.endswith(('.xls', '.xlsx')):\n",
    "                df = pd.read_excel(file)\n",
    "            else:\n",
    "                print(f\"Unsupported file format: {file}\")\n",
    "                continue\n",
    "            if filepath:\n",
    "                df['FilePath'] = file  # 添加來源檔案路徑欄位\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    # 合併所有 DataFrame\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "# 01_資料預處理\n",
    "def filter_ticket_data(filepath, \n",
    "                       selectdate_start, \n",
    "                       selectdate_end, \n",
    "                       outputfolder,\n",
    "                       skiprows=1, \n",
    "                       chunksize=1000,\n",
    "                        on_time_column = 'BoardingTime', \n",
    "                       off_time_column = 'DeboardingTime', \n",
    "                       infodate_column = 'InfoDate',):\n",
    "    \"\"\"\n",
    "    分批讀取大型票證 CSV，依上車時間欄位做日期篩選後輸出新的 CSV。\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        原始 CSV 路徑\n",
    "    on_time_column : str\n",
    "        上車時間欄位名稱\n",
    "    off_time_column : str\n",
    "        下車時間欄位名稱（保留未來擴充）\n",
    "    selectdate_start : str\n",
    "        篩選起始日期（YYYY-MM-DD）\n",
    "    selectdate_end : str\n",
    "        篩選結束日期（YYYY-MM-DD）\n",
    "    outputfolder : str\n",
    "        最終輸出 CSV 的資料夾路徑\n",
    "    skiprows : int\n",
    "        讀取 CSV 時跳過的列\n",
    "    chunksize : int\n",
    "        每批讀取筆數\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    outputpath : str\n",
    "        最終輸出 CSV 的完整路徑\n",
    "    \"\"\"\n",
    "\n",
    "    # 建立輸出資料夾（如不存在）\n",
    "    os.makedirs(outputfolder, exist_ok=True)\n",
    "\n",
    "    # 產生輸出檔名\n",
    "    filename = os.path.basename(filepath).replace(\n",
    "        \".csv\", f\"_{selectdate_start}_to_{selectdate_end}.csv\"\n",
    "    )\n",
    "    outputpath = os.path.join(outputfolder, filename)\n",
    "\n",
    "    # 日期轉 datetime\n",
    "    start = pd.to_datetime(selectdate_start)\n",
    "    end   = pd.to_datetime(selectdate_end)\n",
    "\n",
    "    # 分批讀取\n",
    "    chunks = pd.read_csv(filepath, skiprows=skiprows, chunksize=chunksize)\n",
    "    first_chunk = True\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # 轉成 datetime\n",
    "        # chunk[on_time_column] = pd.to_datetime(chunk[on_time_column], errors='coerce')\n",
    "        # chunk[off_time_column] = pd.to_datetime(chunk[off_time_column], errors='coerce')\n",
    "        chunk[infodate_column] = pd.to_datetime(chunk[infodate_column], errors='coerce')\n",
    "\n",
    "        # 日期篩選\n",
    "        # mask = (\n",
    "        #     ((chunk[on_time_column]  >= start) & (chunk[on_time_column]  <= end)) |\n",
    "        #     ((chunk[off_time_column] >= start) & (chunk[off_time_column] <= end))\n",
    "        # )    \n",
    "        # mask = (chunk[on_time_column] >= start) & (chunk[on_time_column] <= end)\n",
    "        mask = (chunk[infodate_column] >= start) & (chunk[infodate_column] <= end)\n",
    "        filtered_chunk = chunk[mask]\n",
    "\n",
    "        if filtered_chunk.empty:\n",
    "            continue\n",
    "\n",
    "        # 寫入 CSV\n",
    "        filtered_chunk.to_csv(\n",
    "            outputpath,\n",
    "            mode='w' if first_chunk else 'a',\n",
    "            header=first_chunk,\n",
    "            index=False,\n",
    "            encoding='utf-8-sig'\n",
    "        )\n",
    "        first_chunk = False\n",
    "\n",
    "    return outputpath\n",
    "\n",
    "# def tickets_cleaning(\n",
    "#     tickets, \n",
    "#     on_time_column='on_time_column', \n",
    "#     off_time_column='off_time_column', \n",
    "#     getonstop='GetOnStop', \n",
    "#     getoffstop='GetOffStop', \n",
    "#     getonseq='GetOnSeq', \n",
    "#     getoffseq='GetOffSeq'):\n",
    "#     \"\"\"\n",
    "#     清理票證資料，篩選出符合條件的票證並輸出統計結果。\n",
    "#     可以用於檢查票證資料的正確性。\n",
    "#     \"\"\"\n",
    "#     # 原始票證數量\n",
    "#     original_count = len(tickets)\n",
    "\n",
    "#     # 建立篩選條件\n",
    "#     valid_conditions = (\n",
    "#         (tickets[on_time_column] < tickets[off_time_column]) &  # 上車時間早於下車時間\n",
    "#         (tickets[getonstop] != tickets[getoffstop]) &  # 上下車站不同\n",
    "#         (tickets[getonseq] < tickets[getoffseq])  # 上下車序正確\n",
    "#     )\n",
    "\n",
    "#     # 檢查每個條件的異常數量\n",
    "#     late_count = (tickets[on_time_column] > tickets[off_time_column]).sum()\n",
    "#     same_stop_count = (tickets[getonstop] == tickets[getoffstop]).sum()\n",
    "#     seq_error_count = (tickets[getonseq] >= tickets[getoffseq]).sum()\n",
    "    \n",
    "\n",
    "#     # 篩選出符合條件的票證\n",
    "#     cleaned_tickets = tickets[valid_conditions]\n",
    "#     canuse_count = len(cleaned_tickets)\n",
    "\n",
    "#     # 統計結果\n",
    "#     output = {\n",
    "#         '原始票證數量': original_count,\n",
    "#         '資料正常':canuse_count, \n",
    "#         '資料異常 - 上車晚於下車': late_count,\n",
    "#         '資料異常 - 同站上下車': same_stop_count,\n",
    "#         '資料異常 - 上下車次序錯誤': seq_error_count\n",
    "#     }\n",
    "\n",
    "#     correctrate = round((canuse_count / original_count) * 100, 1)\n",
    "#     return cleaned_tickets, output, correctrate\n",
    "\n",
    "def tickets_cleaning(\n",
    "    tickets,\n",
    "    on_time_column='BoardingTime',\n",
    "    off_time_column='DeboardingTime',\n",
    "    getonstop='BoardingStopUID',\n",
    "    getoffstop='DeboardingStopUID',\n",
    "    getonseq='BoardingStopSequence',\n",
    "    getoffseq='DeboardingStopSequence'):\n",
    "\n",
    "    n = len(tickets)\n",
    "\n",
    "    # ---- 型別轉換（你不把缺值當異常，但比較要正確）----\n",
    "    on_time  = pd.to_datetime(tickets[on_time_column], errors='coerce')\n",
    "    off_time = pd.to_datetime(tickets[off_time_column], errors='coerce')\n",
    "    on_seq   = pd.to_numeric(tickets[getonseq], errors='coerce')\n",
    "    off_seq  = pd.to_numeric(tickets[getoffseq], errors='coerce')\n",
    "    on_stop  = tickets[getonstop]\n",
    "    off_stop = tickets[getoffstop]\n",
    "\n",
    "    # ---- 能確定的三種異常（缺值不算異常）----\n",
    "    m_time_rev  = (on_time > off_time)               # 上車晚於下車\n",
    "    m_same_stop = (on_stop == off_stop)              # 同站上下車\n",
    "    m_seq_err   = (on_seq >= off_seq)                # 上序 >= 下序\n",
    "\n",
    "    # ---- 資料正常（只有確定異常才算異常，其餘都正常）----\n",
    "    m_ok = ~(m_time_rev | m_same_stop | m_seq_err)\n",
    "\n",
    "    cleaned = tickets[m_ok].copy()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # ⭐⭐ 依你的要求：新增 ErrorMsg 欄位，描述缺哪些資料（但不當異常）\n",
    "    # ---------------------------------------------------------\n",
    "    miss_off_time = off_time.isna()\n",
    "    miss_off_stop = off_stop.isna()\n",
    "\n",
    "    def combine_err(row):\n",
    "        msgs = []\n",
    "        if row['miss_off_time']:\n",
    "            msgs.append(\"沒有下車刷卡時間\")\n",
    "        if row['miss_off_stop']:\n",
    "            msgs.append(\"沒有下車站點資料\")\n",
    "        return \"；\".join(msgs)\n",
    "\n",
    "    temp_df = pd.DataFrame({\n",
    "        \"miss_off_time\": miss_off_time,\n",
    "        \"miss_off_stop\": miss_off_stop\n",
    "    })\n",
    "\n",
    "    cleaned[\"ErrorMsg\"] = temp_df.loc[cleaned.index].apply(combine_err, axis=1)\n",
    "    # 若沒有錯誤，改成空字串\n",
    "    cleaned[\"ErrorMsg\"] = cleaned[\"ErrorMsg\"].replace(\"\", \"\")\n",
    "\n",
    "    # ---- 統計輸出 ----\n",
    "    output = {\n",
    "        '原始票證數量': int(n),\n",
    "        '資料正常': int(m_ok.sum()),\n",
    "        '資料異常 - 上車晚於下車': int(m_time_rev.sum()),\n",
    "        '資料異常 - 同站上下車': int(m_same_stop.sum()),\n",
    "        '資料異常 - 上下車次序錯誤': int(m_seq_err.sum()),\n",
    "        # 額外統計（可選）：缺哪些資料\n",
    "        '資訊缺失 - 沒有下車刷卡時間': int(miss_off_time.sum()),\n",
    "        '資訊缺失 - 沒有下車站點資料': int(miss_off_stop.sum())\n",
    "    }\n",
    "\n",
    "    correctrate = round((output['資料正常'] / n) * 100, 2) if n else 0.0\n",
    "    return cleaned, output, correctrate\n",
    "\n",
    "def mark_ticket_errors(\n",
    "    tickets, \n",
    "    on_time_column='on_time_column', \n",
    "    off_time_column='off_time_column', \n",
    "    getonstop='GetOnStop', \n",
    "    getoffstop='GetOffStop', \n",
    "    getonseq='GetOnSeq', \n",
    "    getoffseq='GetOffSeq'):\n",
    "    \"\"\"\n",
    "    在票證資料上貼三種錯誤標籤，為 0/1。\n",
    "    不做篩選，不刪資料，只新增欄位。\n",
    "    \"\"\"\n",
    "    tickets['error_time'] = (tickets[on_time_column] > tickets[off_time_column]).astype(int)\n",
    "    tickets['error_same_stop'] = (tickets[getonstop] == tickets[getoffstop]).astype(int)\n",
    "    tickets['error_seq'] = (tickets[getonseq] >= tickets[getoffseq]).astype(int)\n",
    "\n",
    "    tickets['error'] = (\n",
    "        (tickets['error_time'] == 1) |\n",
    "        (tickets['error_same_stop'] == 1) |\n",
    "        (tickets['error_seq'] == 1)\n",
    "    ).astype(int)\n",
    "\n",
    "    return tickets\n",
    "\n",
    "def export_ticketcorrectrate(filename, output, correctrate, txt_path):\n",
    "\n",
    "    # 運算時間\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # 判斷檔案是否已存在\n",
    "    file_exists = os.path.exists(txt_path)\n",
    "\n",
    "    # 若檔案不存在 → 用 w (寫入 header)\n",
    "    # 若檔案存在 → 用 a (不寫 header)\n",
    "    mode = \"a\" if file_exists else \"w\"\n",
    "\n",
    "    with open(txt_path, mode, encoding=\"utf-8\") as f:\n",
    "\n",
    "        # 如果是新檔案，寫入 header\n",
    "        if not file_exists:\n",
    "            f.write(\"filename,timestamp,key,value\\n\")\n",
    "\n",
    "        # 寫入 output 每筆資料\n",
    "        for key, value in output.items():\n",
    "            f.write(f\"{filename},{timestamp},{key},{value}\\n\")\n",
    "\n",
    "        # 寫入正確率\n",
    "        f.write(f\"{filename},{timestamp},正確率,{correctrate}\\n\")\n",
    "\n",
    "    print(f\"TXT (CSV 格式) 已輸出：{txt_path}\")\n",
    "\n",
    "def get_stop_fromtickets(df):\n",
    "    \"\"\"\n",
    "    從票證資料中提取所有上下車站點資訊，並合併成一個包含所有站點的 DataFrame。\n",
    "    用於檢查票種的站點是否為可用的站點，因為有站點才有辦法核對到GIS。\n",
    "    \n",
    "    參數:\n",
    "    df (DataFrame): 包含票證資料的 DataFrame，需包含上下車站點相關欄位。\n",
    "    \n",
    "    回傳:\n",
    "    DataFrame: 包含所有上下車站點資訊的 DataFrame。\n",
    "    \"\"\"\n",
    "    \n",
    "     # 選取需要的欄位\n",
    "    select_columns = ['Authority', 'OperatorNo',  \n",
    "                    'RouteUID', 'RouteName', 'SubRouteUID', 'SubRouteName', 'Direction']\n",
    "    boarding_stop_columns = ['BoardingStopUID', 'BoardingStopName', 'BoardingStopSequence']\n",
    "    deboarding_stop_columns = ['DeboardingStopUID', 'DeboardingStopName', 'DeboardingStopSequence']\n",
    "\n",
    "    # 取上車資料\n",
    "    dfboarding =  df[select_columns + boarding_stop_columns]\n",
    "    dfboarding[select_columns + boarding_stop_columns] = dfboarding[select_columns + boarding_stop_columns].fillna('-99')\n",
    "    dfboarding.columns = dfboarding.columns.str.replace('Boarding', '')\n",
    "    dfboarding['OnorOff'] = 'On'\n",
    "\n",
    "    # 取下車資料\n",
    "    dfdeboarding =  df[select_columns + deboarding_stop_columns]\n",
    "    dfdeboarding[select_columns + deboarding_stop_columns] = dfdeboarding[select_columns+ deboarding_stop_columns].fillna('-99')\n",
    "    dfdeboarding.columns = dfdeboarding.columns.str.replace('Deboarding', '')\n",
    "    dfdeboarding['OnorOff'] = 'Off'\n",
    "    # 合併上下車站點資料\n",
    "    df_stops = pd.concat([dfboarding, dfdeboarding], ignore_index=True)\n",
    "    \n",
    "    df_stops = (\n",
    "        df_stops\n",
    "        .fillna(-99)\n",
    "        .groupby(df_stops.columns.tolist())\n",
    "        .size()\n",
    "        .reset_index(name='Count')\n",
    "    )\n",
    "\n",
    "    return df_stops\n",
    "\n",
    "def match_stop_coordinates(\n",
    "    dfstop, \n",
    "    stop_gdf, \n",
    "    col_uid=\"StopUID\", \n",
    "    col_name=\"StopName\", \n",
    "    col_lat=\"Lat\", \n",
    "    col_lon=\"Lon\"):\n",
    "    \"\"\"\n",
    "    進行兩階段站點比對，並將所有原本 print 的文字改成 text 文字回傳：\n",
    "    回傳：\n",
    "        dfcount_final : 二階段比對後結果 DataFrame\n",
    "        text : 報表文字（取代 print）\n",
    "    \"\"\"\n",
    "\n",
    "    text_output = []\n",
    "\n",
    "    # 第一次比對：比對 StopUID 與 StopName\n",
    "    dfcount = pd.merge(\n",
    "        dfstop,\n",
    "        stop_gdf[[col_uid, col_name, col_lon, col_lat]].drop_duplicates(subset=[col_uid, col_name]),\n",
    "        on=[col_uid, col_name],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    total = dfstop[\"Count\"].sum()\n",
    "    abnormal = dfcount[(dfcount[col_lon].isna()) | (dfcount[col_lat].isna())][\"Count\"].sum()\n",
    "\n",
    "    text_output.append(\"第一次比對結果\")\n",
    "    text_output.append(f\"總共有幾筆資料: {total:,}\")\n",
    "    text_output.append(f\"沒有對應經緯度座標的資料異常數量: {abnormal:,}\")\n",
    "    text_output.append(f\"影響比例: {abnormal / total:.4%}\")\n",
    "    text_output.append(\"============================\")\n",
    "\n",
    "    # 第二次比對：只比對 StopUID\n",
    "    dfcount_2ndround = dfcount[(dfcount[col_lon].isna()) | (dfcount[col_lat].isna())].copy()\n",
    "\n",
    "    dfcount_2ndround = pd.merge(\n",
    "        dfcount_2ndround.drop(columns=[col_lon, col_lat]),\n",
    "        stop_gdf[[col_uid, col_lon, col_lat, col_name]].drop_duplicates(subset=[col_uid]),\n",
    "        on=[col_uid],\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_gdf\")\n",
    "    )\n",
    "\n",
    "    total_2ndround = dfcount_2ndround[\"Count\"].sum()\n",
    "    abnormal_2ndround = dfcount_2ndround[(dfcount_2ndround[col_lon].isna()) | (dfcount_2ndround[col_lat].isna())][\"Count\"].sum()\n",
    "\n",
    "    text_output.append(\"第二次比對結果\")\n",
    "    text_output.append(f\"第二次比對 - 總共有幾筆資料: {total_2ndround:,}\")\n",
    "    text_output.append(f\"第二次比對 - 沒有對應經緯度座標的資料異常數量: {abnormal_2ndround:,}\")\n",
    "    text_output.append(f\"第二次比對 - 影響比例: {abnormal_2ndround / total_2ndround:.4%}\")\n",
    "    text_output.append(f\"第二次比對 - 影響佔可用票證的原始比例: {abnormal_2ndround / total:.4%}\")\n",
    "    text_output.append(\"============================\")\n",
    "\n",
    "    # 最終合併：第一次成功 + 第二次比對結果\n",
    "    dfcount_final = pd.concat(\n",
    "        [dfcount[~((dfcount[col_lon].isna()) | (dfcount[col_lat].isna()))], \n",
    "         dfcount_2ndround],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # 將文字合成一個字串\n",
    "    text = \"\\n\".join(text_output)\n",
    "\n",
    "    return dfcount_final, text\n",
    "\n",
    "\n",
    "# 02_資料分析處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1428724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00_Setup 所有全域函數\n",
    "\n",
    "# 1.) 設定篩選日期區間\n",
    "selectdate_start = '2024-10-01'\n",
    "selectdate_end = '2024-11-30'\n",
    "\n",
    "# 2.) 建立輸出資料夾\n",
    "selecttime_ticket_folder = create_folder(os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '01_指定時間區間票證資料')) # 建立01-01 指定時間區間票證資料夾\n",
    "checkok_ticketfolder = create_folder(os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '02_過濾可用票證資料')) # 建立01-02 過濾可用票證資料夾\n",
    "check_stopfolder = create_folder(os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '03_所有使用到的點位')) # 建立01-03 所有使用到的點位資料夾\n",
    "reformat_folder = create_folder(os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '04_計算交通量格式')) # 建立01-03 所有使用到的點位資料夾\n",
    "\n",
    "hourlycount_folder = create_folder(os.path.join(os.getcwd(), '..', '02_初步分析', '01_分時計次')) # 建立01-03 所有使用到的點位資料夾\n",
    "dailybetweenstops_folder = create_folder(os.path.join(os.getcwd(), '..', '02_初步分析', '02_全日站間量'))\n",
    "od_folder = create_folder(os.path.join(os.getcwd(), '..', '02_初步分析', '03_OD起迄量'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b02f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預處理01: 指定時間區間票證資料切分\n",
    "def pre01_split_ticket_with_day(selectdate_start, selectdate_end, outputfolder):\n",
    "        orginal_ticket_files = [\n",
    "                                r'D:\\B-Project\\2025\\6800\\Technical\\12票證資料\\2024_2025\\公路客運電子票證資料(TO1A)\\公路客運電子票證資料(TO1A).csv', \n",
    "                                r'D:\\B-Project\\2025\\6800\\Technical\\12票證資料\\2024_2025\\新北市公車電子票證資料(TO1A)\\新北市公車電子票證資料(TO1A).csv', \n",
    "                                r'D:\\B-Project\\2025\\6800\\Technical\\12票證資料\\2024_2025\\桃園市公車電子票證資料(TO1A)\\桃園市公車電子票證資料(TO1A).csv', \n",
    "                                ]\n",
    "        for file in orginal_ticket_files:\n",
    "                output = filter_ticket_data(\n",
    "                        filepath = file,\n",
    "                        infodate_column = 'InfoDate',\n",
    "                        selectdate_start = selectdate_start,\n",
    "                        selectdate_end = selectdate_end,\n",
    "                        outputfolder = outputfolder,\n",
    "                        skiprows = 1,\n",
    "                        chunksize = 1000\n",
    "                        )\n",
    "                print(\"輸出路徑：\", output)\n",
    "\n",
    "# pre01_split_ticket_with_day(selectdate_start, selectdate_end, selecttime_ticket_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 額外處理 -> 為了找到是否有問題的\n",
    "marked_ticketfolder = create_folder(\n",
    "    os.path.join(os.getcwd(), '..', '01_初步篩選整理票證', '01-01_指定時間區間票證資料_但有錯誤標記')\n",
    ")\n",
    "\n",
    "selecttime_ticket_files = findfiles(selecttime_ticket_folder, filetype='.csv', recursive=False)\n",
    "selecttime_ticket_files = [f for f in selecttime_ticket_files if 'TO1' in f]\n",
    "\n",
    "for file in selecttime_ticket_files:\n",
    "    marked_output_file = os.path.join(\n",
    "        marked_ticketfolder,\n",
    "        os.path.basename(file).replace(\".csv\", \"_marked.csv\")\n",
    "    )\n",
    "\n",
    "    # 如果 mark_ticket_errors 需要全表上下文，改成 chunksize=None\n",
    "    reader = pd.read_csv(file, chunksize=1000)\n",
    "\n",
    "    first_chunk = True\n",
    "    for chunk in reader:\n",
    "        output = mark_ticket_errors(\n",
    "            tickets=chunk, \n",
    "            on_time_column='BoardingTime',\n",
    "            off_time_column='DeboardingTime',\n",
    "            getonstop='BoardingStopUID',\n",
    "            getoffstop='DeboardingStopUID',\n",
    "            getonseq='BoardingStopSequence',\n",
    "            getoffseq='DeboardingStopSequence'\n",
    "        )\n",
    "\n",
    "        output.to_csv(\n",
    "            marked_output_file,\n",
    "            mode='w' if first_chunk else 'a',\n",
    "            header=first_chunk,\n",
    "            index=False,\n",
    "            encoding='utf-8-sig'\n",
    "        )\n",
    "        first_chunk = False  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3d92724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預處理02: 指定時間區間票證資料切分\n",
    "def pre02_get_correct_tickets(selecttime_ticket_folder, checkok_ticketfolder):\n",
    "\n",
    "    selecttime_ticket_files = findfiles(selecttime_ticket_folder, filetype='.csv', recursive=False)\n",
    "    correctratelog_path = os.path.join(checkok_ticketfolder, '客運票證資料正確率記錄.txt')\n",
    "\n",
    "    chunksize = 10000   \n",
    "\n",
    "    for file in selecttime_ticket_files:\n",
    "\n",
    "        print(f\"\\n=== 開始處理：{file} ===\")\n",
    "\n",
    "        # 統計資料累加器\n",
    "        total_stat = Counter()\n",
    "\n",
    "        # 輸出清洗後 CSV 的路徑\n",
    "        cleaned_output_path = os.path.join(\n",
    "            checkok_ticketfolder,\n",
    "            os.path.basename(file).replace(\".csv\", \"_cleaned.csv\")\n",
    "        )\n",
    "\n",
    "        first_chunk = True  # 控制 header\n",
    "\n",
    "        # 分批讀取整個檔案\n",
    "        for chunk in pd.read_csv(file, chunksize=chunksize, encoding='utf-8-sig'):\n",
    "\n",
    "            # 跑你自己的清洗函數\n",
    "            cleaned_df, correct_stat_info, correctrate_chunk = tickets_cleaning(\n",
    "                chunk,\n",
    "                on_time_column='BoardingTime',\n",
    "                off_time_column='DeboardingTime',\n",
    "                getonstop='BoardingStopUID',\n",
    "                getoffstop='DeboardingStopUID',\n",
    "                getonseq='BoardingStopSequence',\n",
    "                getoffseq='DeboardingStopSequence'\n",
    "            )\n",
    "\n",
    "            # 累加統計\n",
    "            total_stat.update(correct_stat_info)\n",
    "\n",
    "            # 將清洗後的 cleaned_df 分批寫入新 CSV\n",
    "            if not cleaned_df.empty:\n",
    "                cleaned_df.to_csv(\n",
    "                    cleaned_output_path,\n",
    "                    mode='w' if first_chunk else 'a',\n",
    "                    header=first_chunk,\n",
    "                    index=False,\n",
    "                    encoding='utf-8-sig'\n",
    "                )\n",
    "                first_chunk = False\n",
    "\n",
    "        # -------- 整份 CSV 的整體正確率 --------\n",
    "        original_count = total_stat.get('原始票證數量', 0)\n",
    "        canuse_count   = total_stat.get('資料正常', 0)\n",
    "\n",
    "        if original_count > 0:\n",
    "            final_correctrate = round(canuse_count / original_count * 100, 2)\n",
    "        else:\n",
    "            final_correctrate = 0.0\n",
    "\n",
    "        # -------- 寫入 TXT（CSV 格式） --------\n",
    "        export_ticketcorrectrate(\n",
    "            filename=file,\n",
    "            output=dict(total_stat),\n",
    "            correctrate=final_correctrate,\n",
    "            txt_path=correctratelog_path\n",
    "        )\n",
    "\n",
    "        print(f\"清洗後資料輸出：{cleaned_output_path}\")\n",
    "\n",
    "# pre02_get_correct_tickets(selecttime_ticket_folder, checkok_ticketfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f70d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預處理03: 確認所有站點的經緯度在TDX都可以被核對出來\n",
    "\n",
    "def pre03_findstops(checkok_ticketfolder, \n",
    "                    seqfolder = r\"D:\\B-Project\\2025\\6800\\Technical\\12票證資料\\TicketAnalysis\\00_TDX資料下載\\01公車站序資料\"):\n",
    "\n",
    "    files = findfiles(checkok_ticketfolder)\n",
    "    files = [f for f in files if 'TO1' in f]\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file, encoding='utf-8-sig')\n",
    "        stop = get_stop_fromtickets(df)\n",
    "        stop['file_source'] = os.path.basename(file)\n",
    "\n",
    "        outputfilename = os.path.join(check_stopfolder, os.path.basename(file).replace('_cleaned.csv', '_stops.csv'))\n",
    "        stop.to_csv(outputfilename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"站點資料輸出：{outputfilename}\")\n",
    "\n",
    "    df_stop = read_combined_dataframe(findfiles(check_stopfolder, filetype='csv', recursive=False), filepath=False)\n",
    "\n",
    "    df_seq = read_combined_dataframe(findfiles(seqfolder, \n",
    "                                            filetype='csv', \n",
    "                                            recursive=False), filepath=False)\n",
    "    df_stopfromseq = df_seq[['StopUID', 'StopName_Zh', 'PositionLon', 'PositionLat']].drop_duplicates(subset=['StopUID']).sort_values(['StopUID'])\n",
    "\n",
    "    df_final, report_text = match_stop_coordinates(\n",
    "        dfstop=df_stop.copy().rename(columns = {'StopName':'StopName_Zh'}),\n",
    "        stop_gdf=df_stopfromseq,\n",
    "        col_uid=\"StopUID\",\n",
    "        col_name=\"StopName_Zh\",\n",
    "        col_lat=\"PositionLat\",\n",
    "        col_lon=\"PositionLon\"\n",
    "    )\n",
    "\n",
    "    print(report_text)\n",
    "\n",
    "\n",
    "\n",
    "    # a = df_final[((df_final['PositionLon'].isna()) | (df_final['PositionLat'].isna())) & (df_final['StopUID'] != \"-99\")][['StopUID', 'StopName_Zh']].drop_duplicates()\n",
    "    # a['Auth'] = a['StopUID'].str[:3]\n",
    "    # a.sort_values(['Auth'])\n",
    "\n",
    "# pre03_findstops(checkok_ticketfolder, seqfolder = r\"D:\\B-Project\\2025\\6800\\Technical\\12票證資料\\TicketAnalysis\\00_TDX資料下載\\01公車站序資料\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "daecf5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預處理04: 加上必要欄位 (平假日欄位、刪除不重要的欄位）\n",
    "\n",
    "def add_weekdayandweekendcolumns(df, \n",
    "                                 timecolumns='InfoDate',\n",
    "                                 filterdate=None):\n",
    "    \"\"\"\n",
    "    將 DataFrame 中的時間欄位轉換為日期時間格式，新增 DaysofWeek 和 WDWK 欄位，\n",
    "    並可選擇性地過濾掉特定日期。\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): 原始 DataFrame。\n",
    "        timecolumns (str): 包含日期的欄位名稱，預設為 'InfoDate'。\n",
    "        filterdate (list/None): 要過濾掉的日期字串列表 (例如 ['YYYY-MM-DD'])。\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 處理後的 DataFrame。\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. 將時間欄位轉換為 datetime\n",
    "    df[timecolumns] = pd.to_datetime(df[timecolumns], errors='coerce')\n",
    "\n",
    "    # 2. 新增 'DaysofWeek' 欄位\n",
    "    \n",
    "    df['DaysofWeek'] = df[timecolumns].dt.dayofweek # .dt.dayofweek 會回傳：0=週一, 1=週二, ..., 6=週日\n",
    "\n",
    "    # 3. 處理過濾日期 (如果 filterdate 不是 None 且有內容)\n",
    "    if filterdate and len(filterdate) > 0:\n",
    "        # 將 filterdate 列表轉換為 datetime 格式，以便進行比較\n",
    "        filter_dates_dt = pd.to_datetime(filterdate)\n",
    "        \n",
    "        # 找出不在 filter_dates_dt 中的日期 (布林遮罩)\n",
    "        # .dt.normalize() 將日期時間的時間部分設為 00:00:00，確保只比較日期\n",
    "        filter_mask = ~df[timecolumns].dt.normalize().isin(filter_dates_dt)\n",
    "        \n",
    "        # 套用遮罩，只保留不在過濾列表中的資料\n",
    "        df = df[filter_mask].copy()\n",
    "\n",
    "    # 4. 新增 'WDWK' 欄位\n",
    "    # .dt.dayofweek 回傳：0=週一, 1=週二, 2=週三, 3=週四, 4=週五, 5=週六, 6=週日\n",
    "    \n",
    "    # 定義條件：\n",
    "    # WDWK = 1 (週二=1, 週三=2, 週四=3)\n",
    "    wdwk_1_condition = df['DaysofWeek'].isin([1, 2, 3])\n",
    "    \n",
    "    # WDWK = -1 (週六=5, 週日=6)\n",
    "    wdwk_neg1_condition = df['DaysofWeek'].isin([5, 6])\n",
    "    \n",
    "    # 使用 np.select (比多個 if/elif 判斷更快)\n",
    "    \n",
    "    df['WDWK'] = np.select(\n",
    "        [wdwk_1_condition, wdwk_neg1_condition], # 條件列表\n",
    "        [1, 0],                                # 對應的值\n",
    "        default=-1                               # 預設值 (其他日子=1)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def must_outputformat(df):\n",
    "    df['BoardingTime'] = pd.to_datetime(df['BoardingTime'], errors='coerce')\n",
    "    df['DeboardingTime'] = pd.to_datetime(df['DeboardingTime'], errors='coerce')\n",
    "    df['BoardinngDate'] = df['BoardingTime'].dt.date\n",
    "    df['DeboardingDate'] = df['DeboardingTime'].dt.date\n",
    "    df['BoardingHour'] = df['BoardingTime'].dt.hour\n",
    "    df['DeboardingHour'] = df['DeboardingTime'].dt.hour\n",
    "\n",
    "    reindexcolumns = ['Authority', 'OperatorNo', 'HolderType', 'TicketType', 'SubTicketType', \n",
    "                    'RouteUID', 'RouteName', 'SubRouteUID', 'SubRouteName', 'Direction', \n",
    "                    'BoardingStopUID', 'BoardingStopName', 'BoardingStopSequence', 'BoardinngDate',  'BoardingHour', \n",
    "                    'DeboardingStopUID', 'DeboardingStopName', 'DeboardingStopSequence', 'DeboardingDate', 'DeboardingHour',\n",
    "                    'InfoDate', 'DaysofWeek', 'WDWK']\n",
    "\n",
    "    df = df.reindex(columns=reindexcolumns)\n",
    "    return df \n",
    "\n",
    "def pre04_reformat(checkok_ticketfolder, reformat_folder):\n",
    "\n",
    "    filelist = findfiles(checkok_ticketfolder, filetype='csv', recursive=False)\n",
    "\n",
    "    for file in filelist:\n",
    "\n",
    "        reformat_output_file = os.path.join(\n",
    "            reformat_folder,\n",
    "            os.path.basename(file).replace(\"_cleaned.csv\", \"_reformatted.csv\")\n",
    "        )\n",
    "\n",
    "\n",
    "        # 如果 mark_ticket_errors 需要全表上下文，改成 chunksize=None\n",
    "        reader = pd.read_csv(file, chunksize=1000)\n",
    "\n",
    "        first_chunk = True\n",
    "        for chunk in reader:\n",
    "\n",
    "            output = add_weekdayandweekendcolumns(df=chunk,\n",
    "                                            timecolumns= 'InfoDate', \n",
    "                                            filterdate= ['2024-10-09', '2024-10-10', '2024-10-11', '2024-10-12', '2024-10-13', '2024-10-14', '2024-10-15'])\n",
    "            output = must_outputformat(output)\n",
    "\n",
    "            output.to_csv(\n",
    "                reformat_output_file,\n",
    "                mode='w' if first_chunk else 'a',\n",
    "                header=first_chunk,\n",
    "                index=False,\n",
    "                encoding='utf-8-sig'\n",
    "            )\n",
    "            first_chunk = False  \n",
    "\n",
    "pre04_reformat(checkok_ticketfolder, reformat_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da6279df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kjchang\\AppData\\Local\\Temp\\ipykernel_14888\\315726734.py:44: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.36 GiB for an array with shape (14, 22660980) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m returndf:\n\u001b[32m     43\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m df_count\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43manalytics01_hourlycount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreformat_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mhourlycount_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mseqfolder\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mD:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mB-Project\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m2025\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m6800\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mTechnical\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m12票證資料\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mTicketAnalysis\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m00_TDX資料下載\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m01公車站序資料\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mreturndf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36manalytics01_hourlycount\u001b[39m\u001b[34m(reformat_folder, hourlycount_folder, seqfolder, returndf)\u001b[39m\n\u001b[32m     20\u001b[39m df_seq = read_combined_dataframe(findfiles(seqfolder, \n\u001b[32m     21\u001b[39m                                         filetype=\u001b[33m'\u001b[39m\u001b[33mcsv\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     22\u001b[39m                                         recursive=\u001b[38;5;28;01mFalse\u001b[39;00m), filepath=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     23\u001b[39m df_stopfromseq = df_seq[[\u001b[33m'\u001b[39m\u001b[33mStopUID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mStopName_Zh\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPositionLon\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPositionLat\u001b[39m\u001b[33m'\u001b[39m]].drop_duplicates(subset=[\u001b[33m'\u001b[39m\u001b[33mStopUID\u001b[39m\u001b[33m'\u001b[39m]).sort_values([\u001b[33m'\u001b[39m\u001b[33mStopUID\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m df_count = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mdf_stopfromseq\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mStopUID\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPositionLon\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPositionLat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mStopUID\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBoardingStopUID\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPositionLon\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBoardingLon\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPositionLat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBoardingLat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mon\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBoardingStopUID\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m df_count = pd.merge(df_count, \n\u001b[32m     31\u001b[39m                     df_stopfromseq[[\u001b[33m'\u001b[39m\u001b[33mStopUID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPositionLon\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPositionLat\u001b[39m\u001b[33m'\u001b[39m]].rename(columns = {\u001b[33m'\u001b[39m\u001b[33mStopUID\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mDeboardingStopUID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPositionLon\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mDeboardingLon\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPositionLat\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mDeboardingLat\u001b[39m\u001b[33m'\u001b[39m}), \n\u001b[32m     32\u001b[39m                     on = \u001b[33m'\u001b[39m\u001b[33mDeboardingStopUID\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     33\u001b[39m                     how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     34\u001b[39m df_count = df_count.reindex(columns= [\u001b[33m'\u001b[39m\u001b[33mDaysofWeek\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mWDWK\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mHolderType\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mRouteUID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mRouteName\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSubRouteUID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSubRouteName\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     35\u001b[39m                                     \u001b[33m'\u001b[39m\u001b[33mBoardingStopUID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mBoardingStopName\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mBoardingStopSequence\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mBoardinngDate\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mBoardingHour\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mBoardingLon\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mBoardingLat\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     36\u001b[39m                                     \u001b[33m'\u001b[39m\u001b[33mDeboardingStopUID\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mDeboardingStopName\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDeboardingStopSequence\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDeboardingDate\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDeboardingHour\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDeboardingLon\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDeboardingLat\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     37\u001b[39m                                     \u001b[33m'\u001b[39m\u001b[33mFilePath\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCount\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     op = _MergeOperation(\n\u001b[32m    171\u001b[39m         left_df,\n\u001b[32m    172\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m         validate=validate,\n\u001b[32m    183\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:888\u001b[39m, in \u001b[36m_MergeOperation.get_result\u001b[39m\u001b[34m(self, copy)\u001b[39m\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right = \u001b[38;5;28mself\u001b[39m._indicator_pre_merge(\u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right)\n\u001b[32m    886\u001b[39m join_index, left_indexer, right_indexer = \u001b[38;5;28mself\u001b[39m._get_join_info()\n\u001b[32m--> \u001b[39m\u001b[32m888\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[38;5;28mself\u001b[39m._merge_type)\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.indicator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:879\u001b[39m, in \u001b[36m_MergeOperation._reindex_and_concat\u001b[39m\u001b[34m(self, join_index, left_indexer, right_indexer, copy)\u001b[39m\n\u001b[32m    877\u001b[39m left.columns = llabels\n\u001b[32m    878\u001b[39m right.columns = rlabels\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m result = \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    680\u001b[39m             indexers[ax] = obj_labels.get_indexer(new_labels)\n\u001b[32m    682\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m new_data = \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[32m    688\u001b[39m     new_data._consolidate_inplace()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\concat.py:131\u001b[39m, in \u001b[36mconcatenate_managers\u001b[39m\u001b[34m(mgrs_indexers, axes, concat_axis, copy)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m concat_axis == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     mgrs = \u001b[43m_maybe_reindex_columns_na_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[32m0\u001b[39m].concat_horizontal(mgrs, axes)\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].nblocks > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\concat.py:230\u001b[39m, in \u001b[36m_maybe_reindex_columns_na_proxy\u001b[39m\u001b[34m(axes, mgrs_indexers, needs_copy)\u001b[39m\n\u001b[32m    220\u001b[39m         mgr = mgr.reindex_indexer(\n\u001b[32m    221\u001b[39m             axes[i],\n\u001b[32m    222\u001b[39m             indexers[i],\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m             use_na_proxy=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[32m    228\u001b[39m         )\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m         mgr = \u001b[43mmgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m     new_mgrs.append(mgr)\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:604\u001b[39m, in \u001b[36mBaseBlockManager.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    601\u001b[39m         res._blklocs = \u001b[38;5;28mself\u001b[39m._blklocs.copy()\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[39m, in \u001b[36mBlockManager._consolidate_inplace\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1783\u001b[39m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[32m   1784\u001b[39m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[32m   1785\u001b[39m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[32m   1786\u001b[39m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[32m   1787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_consolidated():\n\u001b[32m-> \u001b[39m\u001b[32m1788\u001b[39m         \u001b[38;5;28mself\u001b[39m.blocks = \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m         \u001b[38;5;28mself\u001b[39m._is_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1790\u001b[39m         \u001b[38;5;28mself\u001b[39m._known_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[39m, in \u001b[36m_consolidate\u001b[39m\u001b[34m(blocks)\u001b[39m\n\u001b[32m   2267\u001b[39m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   2268\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[32m-> \u001b[39m\u001b[32m2269\u001b[39m     merged_blocks, _ = \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2270\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[32m   2271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2272\u001b[39m     new_blocks = extend_blocks(merged_blocks, new_blocks)\n\u001b[32m   2273\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:2301\u001b[39m, in \u001b[36m_merge_blocks\u001b[39m\u001b[34m(blocks, dtype, can_consolidate)\u001b[39m\n\u001b[32m   2298\u001b[39m     new_values = bvals2[\u001b[32m0\u001b[39m]._concat_same_type(bvals2, axis=\u001b[32m0\u001b[39m)\n\u001b[32m   2300\u001b[39m argsort = np.argsort(new_mgr_locs)\n\u001b[32m-> \u001b[39m\u001b[32m2301\u001b[39m new_values = \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2302\u001b[39m new_mgr_locs = new_mgr_locs[argsort]\n\u001b[32m   2304\u001b[39m bp = BlockPlacement(new_mgr_locs)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 2.36 GiB for an array with shape (14, 22660980) and data type object"
     ]
    }
   ],
   "source": [
    "# 分析01: 確認資料各票種、各路線、平假日、起點、迄點筆數\n",
    "\n",
    "def analytics01_hourlycount(reformat_folder, \n",
    "                            hourlycount_folder, \n",
    "                            seqfolder = r\"D:\\B-Project\\2025\\6800\\Technical\\12票證資料\\TicketAnalysis\\00_TDX資料下載\\01公車站序資料\",\n",
    "                            returndf = True):\n",
    "\n",
    "    files = findfiles(reformat_folder)\n",
    "    files = [f for f in files if 'TO1' in f]\n",
    "    df = read_combined_dataframe(files)\n",
    "\n",
    "    groupbycolumns = [ 'DaysofWeek', 'WDWK','Authority', 'HolderType', \n",
    "                    'RouteUID', 'RouteName', 'SubRouteUID', 'SubRouteName',\n",
    "                    'BoardingStopUID', 'BoardingStopName', 'BoardingStopSequence', 'BoardinngDate', 'BoardingHour',\n",
    "                    'DeboardingStopUID', 'DeboardingStopName', 'DeboardingStopSequence', 'DeboardingDate', 'DeboardingHour', 'FilePath']\n",
    "\n",
    "    df[groupbycolumns] = df[groupbycolumns].fillna('-99')\n",
    "    df_count = df.groupby(groupbycolumns).size().reset_index(name='Count')\n",
    "\n",
    "    df_seq = read_combined_dataframe(findfiles(seqfolder, \n",
    "                                            filetype='csv', \n",
    "                                            recursive=False), filepath=False)\n",
    "    df_stopfromseq = df_seq[['StopUID', 'StopName_Zh', 'PositionLon', 'PositionLat']].drop_duplicates(subset=['StopUID']).sort_values(['StopUID'])\n",
    "\n",
    "    df_count = pd.merge(df_count, \n",
    "                        df_stopfromseq[['StopUID', 'PositionLon', 'PositionLat']].rename(columns = {'StopUID':'BoardingStopUID', 'PositionLon':'BoardingLon', 'PositionLat':'BoardingLat'}), \n",
    "                        on = 'BoardingStopUID', \n",
    "                        how='left')\n",
    "\n",
    "    df_count = pd.merge(df_count, \n",
    "                        df_stopfromseq[['StopUID', 'PositionLon', 'PositionLat']].rename(columns = {'StopUID':'DeboardingStopUID', 'PositionLon':'DeboardingLon', 'PositionLat':'DeboardingLat'}), \n",
    "                        on = 'DeboardingStopUID', \n",
    "                        how='left')\n",
    "    df_count = df_count.reindex(columns= ['DaysofWeek', 'WDWK', 'HolderType', 'RouteUID', 'RouteName', 'SubRouteUID', 'SubRouteName', \n",
    "                                        'BoardingStopUID', 'BoardingStopName', 'BoardingStopSequence','BoardinngDate', 'BoardingHour', 'BoardingLon', 'BoardingLat', \n",
    "                                        'DeboardingStopUID','DeboardingStopName', 'DeboardingStopSequence', 'DeboardingDate', 'DeboardingHour', 'DeboardingLon', 'DeboardingLat', \n",
    "                                        'FilePath', 'Count'])\n",
    "\n",
    "    outputfile = os.path.join(hourlycount_folder, '上下車區分票種分時計次.csv')\n",
    "    df_count.to_csv(outputfile, index=False)\n",
    "\n",
    "    if returndf:\n",
    "        return df_count\n",
    "\n",
    "analytics01_hourlycount(reformat_folder, \n",
    "                        hourlycount_folder, \n",
    "                        seqfolder = r\"D:\\B-Project\\2025\\6800\\Technical\\12票證資料\\TicketAnalysis\\00_TDX資料下載\\01公車站序資料\", \n",
    "                        returndf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279989ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析02: 確認資料起迄OD各站點次數\n",
    "df = pd.read_csv(r\"D:\\B-Project\\2025\\6800\\Technical\\12票證資料\\TicketAnalysis\\02_初步分析\\01_分時計次\\上下車區分票種分時計次.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "820fa6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HolderType</th>\n",
       "      <th>RouteUID</th>\n",
       "      <th>RouteName</th>\n",
       "      <th>SubRouteUID</th>\n",
       "      <th>SubRouteName</th>\n",
       "      <th>BoardingStopUID</th>\n",
       "      <th>BoardingStopName</th>\n",
       "      <th>BoardingStopSequence</th>\n",
       "      <th>BoardinngDate</th>\n",
       "      <th>BoardingHour</th>\n",
       "      <th>...</th>\n",
       "      <th>BoardingLat</th>\n",
       "      <th>DeboardingStopUID</th>\n",
       "      <th>DeboardingStopName</th>\n",
       "      <th>DeboardingStopSequence</th>\n",
       "      <th>DeboardingDate</th>\n",
       "      <th>DeboardingHour</th>\n",
       "      <th>DeboardingLon</th>\n",
       "      <th>DeboardingLat</th>\n",
       "      <th>FilePath</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>NWT10116</td>\n",
       "      <td>242</td>\n",
       "      <td>NWT101160</td>\n",
       "      <td>242</td>\n",
       "      <td>NWT152736</td>\n",
       "      <td>雙和醫院(圓通路)</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>24.991656</td>\n",
       "      <td>NWT20154</td>\n",
       "      <td>南山捷運路</td>\n",
       "      <td>10</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>15.0</td>\n",
       "      <td>121.504814</td>\n",
       "      <td>24.991463</td>\n",
       "      <td>d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>NWT10116</td>\n",
       "      <td>242</td>\n",
       "      <td>NWT101160</td>\n",
       "      <td>242</td>\n",
       "      <td>NWT152736</td>\n",
       "      <td>雙和醫院(圓通路)</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-10-04</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>24.991656</td>\n",
       "      <td>NWT20162</td>\n",
       "      <td>三介廟</td>\n",
       "      <td>19</td>\n",
       "      <td>2024-10-04</td>\n",
       "      <td>17.0</td>\n",
       "      <td>121.513308</td>\n",
       "      <td>24.988375</td>\n",
       "      <td>d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>NWT10116</td>\n",
       "      <td>242</td>\n",
       "      <td>NWT101160</td>\n",
       "      <td>242</td>\n",
       "      <td>NWT152736</td>\n",
       "      <td>雙和醫院(圓通路)</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>24.991656</td>\n",
       "      <td>NWT20158</td>\n",
       "      <td>華夏科技大學</td>\n",
       "      <td>15</td>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>7.0</td>\n",
       "      <td>121.509241</td>\n",
       "      <td>24.982921</td>\n",
       "      <td>d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>NWT10116</td>\n",
       "      <td>242</td>\n",
       "      <td>NWT101160</td>\n",
       "      <td>242</td>\n",
       "      <td>NWT152736</td>\n",
       "      <td>雙和醫院(圓通路)</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>24.991656</td>\n",
       "      <td>NWT20156</td>\n",
       "      <td>中和國中</td>\n",
       "      <td>13</td>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>17.0</td>\n",
       "      <td>121.507580</td>\n",
       "      <td>24.985780</td>\n",
       "      <td>d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>NWT10116</td>\n",
       "      <td>242</td>\n",
       "      <td>NWT101160</td>\n",
       "      <td>242</td>\n",
       "      <td>NWT152736</td>\n",
       "      <td>雙和醫院(圓通路)</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>24.991656</td>\n",
       "      <td>NWT20158</td>\n",
       "      <td>華夏科技大學</td>\n",
       "      <td>15</td>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>17.0</td>\n",
       "      <td>121.509241</td>\n",
       "      <td>24.982921</td>\n",
       "      <td>d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>A</td>\n",
       "      <td>NWT10149</td>\n",
       "      <td>624綠野香坡</td>\n",
       "      <td>NWT101490</td>\n",
       "      <td>624綠野香坡</td>\n",
       "      <td>NWT170054</td>\n",
       "      <td>安和路二段</td>\n",
       "      <td>23</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>24.976061</td>\n",
       "      <td>NWT170062</td>\n",
       "      <td>中興二村</td>\n",
       "      <td>31</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>17.0</td>\n",
       "      <td>121.514860</td>\n",
       "      <td>24.996424</td>\n",
       "      <td>d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>A</td>\n",
       "      <td>NWT10149</td>\n",
       "      <td>624綠野香坡</td>\n",
       "      <td>NWT101490</td>\n",
       "      <td>624綠野香坡</td>\n",
       "      <td>NWT170054</td>\n",
       "      <td>安和路二段</td>\n",
       "      <td>23</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>24.976061</td>\n",
       "      <td>NWT170064</td>\n",
       "      <td>得和路口</td>\n",
       "      <td>33</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>18.0</td>\n",
       "      <td>121.517714</td>\n",
       "      <td>25.000549</td>\n",
       "      <td>d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>A</td>\n",
       "      <td>NWT10149</td>\n",
       "      <td>624綠野香坡</td>\n",
       "      <td>NWT101490</td>\n",
       "      <td>624綠野香坡</td>\n",
       "      <td>NWT170054</td>\n",
       "      <td>安和路二段</td>\n",
       "      <td>23</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>24.976061</td>\n",
       "      <td>NWT170066</td>\n",
       "      <td>福和里</td>\n",
       "      <td>35</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>17.0</td>\n",
       "      <td>121.517121</td>\n",
       "      <td>25.006519</td>\n",
       "      <td>d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>A</td>\n",
       "      <td>NWT10149</td>\n",
       "      <td>624綠野香坡</td>\n",
       "      <td>NWT101490</td>\n",
       "      <td>624綠野香坡</td>\n",
       "      <td>NWT170054</td>\n",
       "      <td>安和路二段</td>\n",
       "      <td>23</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>24.976061</td>\n",
       "      <td>NWT170081</td>\n",
       "      <td>峨眉街口(中醫院區)</td>\n",
       "      <td>51</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>19.0</td>\n",
       "      <td>121.503074</td>\n",
       "      <td>25.044994</td>\n",
       "      <td>d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>A</td>\n",
       "      <td>NWT10149</td>\n",
       "      <td>624綠野香坡</td>\n",
       "      <td>NWT101490</td>\n",
       "      <td>624綠野香坡</td>\n",
       "      <td>NWT170054</td>\n",
       "      <td>安和路二段</td>\n",
       "      <td>23</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>24.976061</td>\n",
       "      <td>NWT170060</td>\n",
       "      <td>景新街</td>\n",
       "      <td>29</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>19.0</td>\n",
       "      <td>121.512485</td>\n",
       "      <td>24.989999</td>\n",
       "      <td>d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HolderType  RouteUID RouteName SubRouteUID SubRouteName BoardingStopUID  \\\n",
       "0              A  NWT10116       242   NWT101160          242       NWT152736   \n",
       "1              A  NWT10116       242   NWT101160          242       NWT152736   \n",
       "2              A  NWT10116       242   NWT101160          242       NWT152736   \n",
       "3              A  NWT10116       242   NWT101160          242       NWT152736   \n",
       "4              A  NWT10116       242   NWT101160          242       NWT152736   \n",
       "...          ...       ...       ...         ...          ...             ...   \n",
       "99995          A  NWT10149   624綠野香坡   NWT101490      624綠野香坡       NWT170054   \n",
       "99996          A  NWT10149   624綠野香坡   NWT101490      624綠野香坡       NWT170054   \n",
       "99997          A  NWT10149   624綠野香坡   NWT101490      624綠野香坡       NWT170054   \n",
       "99998          A  NWT10149   624綠野香坡   NWT101490      624綠野香坡       NWT170054   \n",
       "99999          A  NWT10149   624綠野香坡   NWT101490      624綠野香坡       NWT170054   \n",
       "\n",
       "      BoardingStopName  BoardingStopSequence BoardinngDate  BoardingHour  ...  \\\n",
       "0            雙和醫院(圓通路)                     3    2024-10-01            15  ...   \n",
       "1            雙和醫院(圓通路)                     3    2024-10-04            17  ...   \n",
       "2            雙和醫院(圓通路)                     3    2024-10-07             7  ...   \n",
       "3            雙和醫院(圓通路)                     3    2024-10-07            17  ...   \n",
       "4            雙和醫院(圓通路)                     3    2024-10-07            17  ...   \n",
       "...                ...                   ...           ...           ...  ...   \n",
       "99995            安和路二段                    23    2024-10-01            17  ...   \n",
       "99996            安和路二段                    23    2024-10-01            17  ...   \n",
       "99997            安和路二段                    23    2024-10-01            17  ...   \n",
       "99998            安和路二段                    23    2024-10-01            18  ...   \n",
       "99999            安和路二段                    23    2024-10-01            19  ...   \n",
       "\n",
       "       BoardingLat  DeboardingStopUID DeboardingStopName  \\\n",
       "0        24.991656           NWT20154              南山捷運路   \n",
       "1        24.991656           NWT20162                三介廟   \n",
       "2        24.991656           NWT20158             華夏科技大學   \n",
       "3        24.991656           NWT20156               中和國中   \n",
       "4        24.991656           NWT20158             華夏科技大學   \n",
       "...            ...                ...                ...   \n",
       "99995    24.976061          NWT170062               中興二村   \n",
       "99996    24.976061          NWT170064               得和路口   \n",
       "99997    24.976061          NWT170066                福和里   \n",
       "99998    24.976061          NWT170081         峨眉街口(中醫院區)   \n",
       "99999    24.976061          NWT170060                景新街   \n",
       "\n",
       "      DeboardingStopSequence  DeboardingDate DeboardingHour  DeboardingLon  \\\n",
       "0                         10      2024-10-01           15.0     121.504814   \n",
       "1                         19      2024-10-04           17.0     121.513308   \n",
       "2                         15      2024-10-07            7.0     121.509241   \n",
       "3                         13      2024-10-07           17.0     121.507580   \n",
       "4                         15      2024-10-07           17.0     121.509241   \n",
       "...                      ...             ...            ...            ...   \n",
       "99995                     31      2024-10-01           17.0     121.514860   \n",
       "99996                     33      2024-10-01           18.0     121.517714   \n",
       "99997                     35      2024-10-01           17.0     121.517121   \n",
       "99998                     51      2024-10-01           19.0     121.503074   \n",
       "99999                     29      2024-10-01           19.0     121.512485   \n",
       "\n",
       "       DeboardingLat                                           FilePath Count  \n",
       "0          24.991463  d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...     1  \n",
       "1          24.988375  d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...     1  \n",
       "2          24.982921  d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...     1  \n",
       "3          24.985780  d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...     1  \n",
       "4          24.982921  d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...     1  \n",
       "...              ...                                                ...   ...  \n",
       "99995      24.996424  d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...     1  \n",
       "99996      25.000549  d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...     2  \n",
       "99997      25.006519  d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...     1  \n",
       "99998      25.044994  d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...     1  \n",
       "99999      24.989999  d:\\B-Project\\2025\\6800\\Technical\\12票證資料\\Ticket...     1  \n",
       "\n",
       "[100000 rows x 21 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
